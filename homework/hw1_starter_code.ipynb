{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import aa598.hw1_helper as hw1_helper\n",
    "\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "rc('text', usetex=True) # set to False if latex is not set up on your computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2EAAAE/CAYAAADPF8uRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACH30lEQVR4nO3de3xT9f0/8NdJmvQCtGm5WG5CUxFF8JKCeEGdkqBUUScpuA3c/P6+NMPdvvr92qzOzSHbavt1+36/m9OlOHXKNqGVKWBVGpzzCtJGEURQmnrjItI2baG0aXPO749DQtOkTdrck9fz8aiYk3OSd05Pzyfv8/mc90eQJEkCERERERERRYUi1gEQERERERGlEiZhREREREREUcQkjIiIiIiIKIqYhBEREREREUURkzAiIiIiIqIoYhJGREREREQURUzCiIiIiIiIoohJGBERERERURQxCSMKk6qqKhQVFaGwsBAmkwl2u93r+cLCQq/HDocDZrMZRUVFMBgMPuvHitVqRUlJCXJzc2Gz2WIdDhFRUquurobBYIAgCCgpKRlyXYfDAUEQkJub67edGUptba3n3O5wOIZc12q1wmAwBP3akTCStmg4nzHU9yIKmUREYVNaWir5+7Oqr6+XAEj19fU+z2m1WqmtrS0K0QWvsbFRAiA1NjYOa7v6+vq4+yxERPHOfc4FMOQ5tKysTAIg6fX6Eb1PTU2N3/cYeO6uqamRNBpNzM/nI2mLBvuMkXgvSWK7RyPHnjCiMHJfORx4Jc1isQAAampqvJY7HA5oNBpoNJqoxBeskcTjvppLRETD078dqKioGHS92trakNoLf9v6O3cbjUa0tbXFvG0ayfuPNGa2exRtTMKIwkin0wEAGhoavJbb7XYYjUZs3LjRa7nVaoVer49afJGUl5cHYOQNIBFRKlu2bBm0Wi2qqqr8Pl9VVQWTyeQ514YLz90jx31HoWASRhRGWq0WGo0G9fX1nmXV1dUwmUwwmUxwOBywWq2e5+rr67F8+XLPY/d9ZUVFRTCbzV7LS0pKUFJS4mmg3feU9V828P0MBkNQ49vdr2UwGFBYWOj13v0NFh8A7Nq1C4B8pba6utozHn+obYiI6Az3OdJfImaxWFBWVuazvP/9TG52ux1msxmFhYWora0d8j0Hnrtfe+01mEwmr/uq+r+Hw+Hw/L/7fub+y4qKivzeq1ZdXQ2z2QyTyTRoexCOtihYbPco5mI9HpIo2RiNRkmr1Xoe6/V6z3hxjUYjGY1Gz3M6nc5rPfdYdPeY9pqaGq91+7+uJMlj2PvfG6DX66WysjLP48rKSkmj0QwZb319vddrtLW1ee5t6z82fqj46uvrJaPRKAGQKisrpcrKSqmtrS3gZyIiIklqamqSSktLJUmS24mB5+2amhrPuV2r1Q56zu6/zN85131/srtN8nfubm5u9tx75l6vsbHRs15paanU1NQkNTU1SRqNRtJqtT7L+rdtkiTfy+b+fO74NBqN1+cIR1vk7zP6w3aP4gGTMKIwq6ys9DQATU1NXklX/4Zy4HMDbwbW6XRejZb7ZN5/vcrKSs9j903F/RuepqamQQuCuA18zf7b9V8ebHz9BdqGiIi8kzCLxeL3Ipz73D4wCZMk/+fftra2oBIUf9sOtV7/ZQOTNUnyLVDlrz3p/zndy8PVFgWThLHdo3iQFqEONqKU5b7Hq6GhAfX19TCZTJ7nTCYTqqurUVtbi9bWVq8benU6HWw2m9f9ZK2trZ7/NxqN0Gg0sFgsnkIfTU1NnuEp7mGO7qEP7jHqWq120FK97m3c97INJVB84dqGiCiVlZaWwmw2o6KiAkajEVarFXPnzh32fUeRuk+p/+u6p17pv6yoqMhrffeQeK1W67Xc3VZarVZPuxCptqg/tnsUL5iEEYWZ+8ReX18Pq9WKyspKr+e0Wi0sFgs0Gg3WrVvnec5gMECn03nW739fmVt5eTnMZjMqKyvR0NDgNadMS0sLgDOVGN36v/9A7uTMXaVxKMHEF45tiIhSnftc725DBp7X44W/IiEDl7kTkNbWVq92pn9Ri0i3Rf2x3aN4wcIcRBGg0+lQVVXlVXTDzWQywWq1wm63exqA2tpan4TNn9LSUgDyDc719fVelRXnzZsHAF6FPwJxX5kcWLVxYDGPYOMLdRsiIgLKysqg0WhgMpmg0Wh8epEG03/UQ7xMOjx37lwAvm2TOznT6/URbYsGYrtH8YJJGFEEuJMjd9LUn9Fo9FoHONMY1dbWwuFwoKqqClar1WcYoUajgdFoREVFhWcYSP/X1Wq1KCkp8WpM3K/pj06ng16v91xxBeSGcsOGDV7rBRufe3ubzTasbYiIUpm/82JpaSnsdjvKy8u9lvsb2uZOLKqrqwHICcVwq/K5z93DEcwwu/7tTP/PWVlZicrKSmi12oi0RcHEw3aPYirWN6URJaOamhqf6lD9abVan2IZer1eAiDp9XqpqalJ0uv1kkajkSwWi9d6/gpwuLkrPGm1Wkmr1UpGozFgVaa2tjbJaDRKGo3GU+XJfYOyXq/3xBkovra2Nkmn03mqZw3nMxERpSqLxSLpdDpJo9FIZWVlnnO7u9Kem7uYEwCvKoVupaWlnvO4+5wLQNLpdFJNTY2nXQIgGY1GTwGJgeduf+u5Y3Qva2pq8lpWWlrqUxGw/zJ3fDqdTiorK5PKysp82sBwtEWDfcaB2O5RPBAkSZKilfARERERERGlOg5HJCIiIiIiiiImYURERERERFHEJIyIiIiIiCiKmIQRERERERFFEZMwIiIiIiKiKGISRkREREREFEVMwoiIiIiIiKIoLdYBxDtRFHH48GGMGTMGgiDEOhwiopQiSRI6OzsxadIkKBS8bujGtomIKDbC1S4xCQvg8OHDmDp1aqzDICJKaV988QWmTJkS6zDiBtsmIqLYCrVdYhIWwJgxYwDIOzo7O3vY2/f29mLbtm1YtGgRVCpVuMOLGMYdXYkYdyLGDDDuaAs17o6ODkydOtVzLiZZqG1TqBL1eIwX3H+h4f4LDfdfaFpbW1FQUBByu8QkLAD3MI/s7OwRJ2FZWVnIzs5OqAOdcUdXIsadiDEDjDvawhU3h9x5C7VtClWiHo/xgvsvNNx/oeH+C01vby+A0NslDrAnIiIiIiKKIiZhREREEWS1WlFUVAS73e613OFwwGw2w2w2w2Aw+DxPREQRIIqA86T8bwwl/HBEq9UKs9mMmpoaaLVav+s4HA5UVFQAAGw2GywWy6DrUuJyiRJ2Nrei8biAsc2tuPycCVAqOISJiGLD4XCgqKgIOp0ONpvN5/mFCxdi+/bt0Gg0sFqtMBgMaGpqikGkREQp4OgeYMejwJ7nAFcPoEwH5iwFLrsLyJ8T9XASNgkL1Lj1x4Yu+b289wjWbNmHI+3dAJR4+pMGTMzJwANLZuGG2RNjHR4RpSCNRoPGxkZoNBqfewesVivy8vKg0WgAAHq9Hna7HTabDTqdLgbREhElsT21wKZSQBAAsU9e5uoBPtgI7N4A3FYNzDFGNaSEHY7obtxqamqGXG+oho6Sw8t7j2D1etvpBOyMo+3dWL3ehpf3HolRZESU6txtz0A2m81nRIZWq+WQRCKicDu6R07AJNeZBMxN7JOXbyqV14uihE3CgMEbt/7Y0CU3lyhhzZZ9kPw85162Zss+uER/axARxUZLS4tPG6bRaNDa2hqbgIiIktWOR+UesKEIArDjsejEc1rCDkcM1nAbup6eHvT09Hged3R0AJDLUbpLUg6He5uRbBtLiRL3zuZWnx6w/iQAR9q78c7BY5hfkDfi93GJEho+a8Oxzh5MGJOOudNyw3q/WaLs7/4SMWaAcUdbqHEn2ucNlsPh8Ls8L8//eSrcbVOoEvV4jBfcf6Hh/gtNSu0/SUTanloIA3vABhL7IO2pQV/x/wZM2MK135I+CRtuQ1dRUYE1a9b4LN+2bRuysrJGHEd9ff2It42leI/77aMCAGXA9ba9sRNf75Ow7UsBk0cBk0dJyFUHvjACALtbBGz6VAGH88zKGrWE26aLuGhseHvY4n1/+5OIMQOMO9pGGndXV1eYI4kPhYWF2LVrl8/ywUZ4RKptClWiHo/xgvsvNNx/oUmF/ad09eAmlzOodQWXE6+8+DxcivQh1wtXu5T0SdhwG7ry8nLcc889nscdHR2YOnUqFi1aNOLJmuvr62EwGBJqQrx4i/uU04V3mlvxr4+/Rq9Lwm9uvQAAMLa5FRuaGwJuv+iq+Rg7So27d7ztWabJVOH8iWMwa+IYnD8xG0VnazAlN9Nru1c+/ApPvrPbZ7hju1PAkx8r8YfbL8L1F5wV8ueLt/0djESMGWDc0RZq3O4en2Sj1WqxYcMGr2V2ux16vd7v+uFum0KVqMdjvOD+Cw33X2hSav+1HIS0RwFBClyOXlKqcf2Ntwa8Qt/S0hKW0JI+CRtuQ5eeno70dN8MWKVShXSghrp9rIQ7bpco4d3mVhzr7MaEMRm4tCBv0GF9n7d04dX9X+GfB77GO/YWOPvkPyB1mgJrbpmNLHUaLj9nAs7KTsexjh6/94UJAPJzMnD5ORPwactJLNVNwYeH23Hw2Ak4TvXiHXsr3rHLQ1N/snAG7jacCwBoOdGDrXuO4PfWTwa930wA8OuXDmDxhZPDNjQxEY+TRIwZYNzRNtK4E/GzBsNoNGLVqlWw2+3QarWw2WyDtktA5NqmUMX6/RMd919ouP9Ck7D7TxSBvlNAWiagCFDeovHPQBAJGBRpEOaUQKVWB1w1XPssaZOwqqoqGI3GYTd0FDneZeRlg5WRv7dmN2oav/RaNlmTiWvPG49rZ05A2uk/OqVCwJqbL8Dq9TYIgFfC5E6LHlgyC0qFgMLxo/HbZRcBAHr6XPjkqxP48HA79h3uwIeHO3Dx2RrPtu997sADL3w45Odx32/2bnMrLi8cO5xdQUQpwmq1eqr4mkwmmEwmGI1yGeTt27fDbDZj3rx5aGlpCVjtl4gopQUzz1fHYaCvB8grkB9fdQ9wbD/w+VtDJ2OSBFy2OvKfoZ+ETsKGatzcEzJrtVo2dHHAXUZ+YK/SkfZufH+9DRdNycG6O+ZiQnYGAOC8idlQKgTMnZaL686bgGvPm4AZE0b7zLUDADfMnojHVuh8Erz8IeYJS09TYvbkHMyenOM33iy1Eufnj8FHRzsDfrZjnYMXBiGi1KbX66HX62GxWHye0+l0bI+IiIIRaJ6vxf8NtHwCNDwBFF4LfPv0KLjsScCdW/1vDwCKNDkBu6066hM2J3QSNlTj1n8yZjZ0sTVUGXm33V+249UDx3D7vLMBACVzp8BYNAU5mcF1+d4weyIMs/LxzsFj2PbGTiy6aj4uP2fCiIcJXnHOOPxiyQX41rodAdedMCZjRO9BRERERAH0n+dr4JdJd0JVd+aeWXR3AM4uQN2vaNEcIzB+plyGfk9tv540o9wDFuUEDEjwJIwSw7sBysi7qZVnxvVmZwx/vK1SIWB+QR5aPpIwf4h7zYJ1aUEeJuZk4Gh795D3m1001X9vGhERERGFyD3PV6CC1JljAePjgPZa/8U18ucAtz4K3PyIfE+ZKiu4MtkRktCTNVNiCHa4Xjjn3QoHpULAA0tmAThzf5mb+/GdV0zHNf/9Gl54/xAkiRNCExEREYWNKMr3gAWa5wsAnCcGT8D6UygA9aiYJmAAkzCKgmCH68XjsD73/Wb5Od6x5edk4LEVOnx4pANfd/bgJ8++j1VPN+BoED1+RERERBSEvlPy0MFguHqA3lORjSeMOByRIsbZJ+LhbQdwwwX5QQ3ru7TA/wTasea+38xfaf3rzjsL2nGj8cg/P4H1o2PYaf8X7rvxfNw+b6rfIiJEREREFKS0TECpBoKZcFmZDqgyA68XJ9gTRhHxeUsXSv70Nqpft+M/NryP+4rPAzD4sD53Gfl4pVQIuLxwLG65eDIuLxzriVWdpsBP9DPw4o+vwkVTNejs6UP5pj34zuM78XlLeGZUJyIiIkpJH78MCMrA6ynS5CIbCXQBnEkYhd3WDw7jxt+/gd1ftiMnU4Wf3zQLSy6aPOSwPn9l5BPJuWeNwabVV+D+G89HhkqBt5ta8M8Dx2IdFhEREVHi6TgCbFgJPPsteUhiIDGY5ytUHI5IYdPd68Larfvw152fAwCKpuXi99+6BJM1ctfwUMP6koFSIeDfr9LCMOssPPPOZ1h52TTPc70uESolr3kQERFRihNFObFKy5SLZAx8rvFJwPpLoKdD7gW74kfAuHOBzT+Kq3m+QsUkjMKi9aQT3163A/uPdkIQgNXXFOJuw7k+iYd7WF8ymzZ2FO6/aZbncZezD0v+8CZuuXgyvn9NIdRpTMaIiIgoxRzdI5eb3/Ncv3m6lgKX3XUmgRIEYO8mOQGbXAQs+b8zz028MK7m+QoVkzAKC02mCpM0mTh+oge/W3Yxrj53fKxDihsvvH8YTV+fxO/qP0bdniOoMl6IC6doPM+7RAk7m1vReFzA2ObWkCaZJiIiIoo7e2rlCZf792S5eoAPNgK7NwA3/x64ZIX8/JL/BZpeBeb9O6Dodz9YnM3zFSomYTRiJ3v6IAhAljoNCoWA35ZchF6XiAnZ8VdqPpZunzcVWWol1mzZh/1HO3HrH9/Cqqu1uFt/Ll47cAxrtuw7PZm1Ek9/0oCJORl4YMmshL9PjoiIiAhH98gJmOTynXDZnZC98ENg4kVyojVuhvwzGPc8XwmO46JoRPYd7sCSR97Ez5//0LMsd5SaCZgfgiDglosno/7uq7HkokkQJcDyLzuurvonvr/edjoBO+NoezdWr7fh5b1HYhQxERERUZjseDSIHisJeOv3UQknXjAJo2GRJAnrd3yGWx99C/avT+Ktg8dx/ESQk+iluLGj0/GHb12CdXfMxYQxahzr9L/f3BeJ1mzZB5fob2Y1IiIiogQgivI9YP2LaQxm3wtykY0UwSSMgtbR3Ysf/u093P/8Xjj7RFx33gTU/eQqjBudHuvQEoph1ln4zW0XDrmOBOBIezfebW6NTlBERERE4dZ3Sr73KxiuHqA3iHL0SYL3hJEPf4Ui9h5qxw//bsMXraeQphDw08Xn4d+uLICCBSRG5GRPEFeEABzr7A68EhEREVE8SsuUqxgGk4gp0wFVZuRjihNMwsjLy3uP+BSKyM/OQHefC46uXkzNy8QfvqXDxVM1sQ41oU0YE9y9c8GuR0RERBR3FAq5DP0HG4cekqhIk8vNJ3C1w+HicETyeHnvEaz2Uyjiq45uOLp6oZumwdYfXcUELAwuLcjDxJwMDHWqmZgjT2ZNRERElJD6eoCZxYHv9ZIkeb6vFMIkjADIQxDXbNnnUzkUkO9PEgAccXRjdDo7T8NBqRDwwBJ5QufBErEfXHsO5wsjIiKixNTTCfzVCGz+EaD/JSAo5R6v/hRp8vLbqhNywuVQMAkjAMC7za0+PWD9sVBE+N0weyIeW6FDfo73kEN34lX9uh1Hh/idEBEREcWlE18DT90ENL8OuHqBSRcDpn8BFy6X7/0C5H8vXC4vn2OMabixwG4NAhB8AQgWigivG2ZPhGFWPt45eAzb3tiJRVfNR8H4bHxr3Q583tqFFX/eiRd/vADpacrAL0ZEREQUa22fAetvA1oOAlljge/UApN18nO3Pgrc/IhcNVGVlVL3gA3EnjACwEIRsaRUCJhfkIeicRLmF+Rhcm4m/vrv8zFZk4l/X1DABIyIiIgSw1f7gCeulxOwnLOBf9t2JgFzUygA9aiUTsAA9oTRaeeeNRpKhTDo5MACgHwWioiaqXlZsN5zDTLVTMCIiIgoAXz1IfDkYqC7HRh/PrByE5A9KdZRxS32hBG6e10wPdM4ZAIGAA8smcVCEVHUPwE7fqIH9/1jD7qcwc0vRkRERBQRogg4T8r/9penBSZcAEydD9xZxwQsAPaEESQJ0GSpMSYjDT/Rz8Cf32j2KtKRn5OBB5bMwg2zJ8YwytQlSRJKn26A7XMHPm/pwuPfnYsMFXvIiIiIKIqO7gF2PArseU6efFmZLs8BdtldcmVDVSbw7WcBhQpQZ8U62rjHJIyQqVbiTyt0+LTlJM6ZMAZ3XlHgVSji8nMmsAcshgRBwM9unIWVf96JNw8exw/+asNjK4qgTmNHNhEREUXBnlpgU6l8H5d70mVXD7D7WeD9vwNLH5crHGbkxDbOBMJvcSns7abjkE5PnpemVOCcCWMA+BaKYAIWe0XTcvHn785DepoC2/cfw90b3kefSwy8IREREVEoju6REzDJdSYBc5NEABKwaZW8HgWNSViK+vu7n+Pb63bC/NwHnkSM4tvlhWNhWVkElVLAi3uOoOy5DyAOch8fERERUVjseDRwJUNBAex4LDrxJAkmYSmoft9X+Nk/5KsVZ2VnQEjxEqGJ5BszJ+CRb+ugVAjYZDuE39V/HOuQiIiIKFmJonwP2MAeMJ/1+uQhi7ywHzQmYSmm8bNW/PBvNogSsGzuFNxjODfWIdEwXX9BPn637CIUjBuF5fOmxjocIiIiSlZ9p+R7v4Lh6gF6T0U2niTCwhwp5OCxTvzbUw3o6ROx8LwJ+M0357AXLEHdcvFk3DA7nxM5ExERUeSkZcpVEINJxJTpcoVECgp7wlLE0fZu3PHnd9F+qheXnK3BI9/WIU3JX38i65+AbfvwKKpfb4phNERERJQ0+pzAjj8B71bLZegVAfptFGlydURe3A8ae8JSxO4vHTjW2QPt+FH483fneU0ETInt4LFOrP6rDS5RglqpwPeuLIh1SERERBSvJBFKV8/pyoYDn5OAj7YA1geAVjugHg3c/jdg94YArykBl62OTLxJil0hKeL6C/LxxPfm4S93Xoq8UepYh0NhdM6EMfjBNwoBAL/csg8bdn0e44iIiIgo7hzdAzy/GmmVU3DTB6uQVjkFeH71mdLyX+wCnrgB2LhSTsBGTQAWrQWmXQncVg0ISt8eMUWavPy2annCZgoae8KSmEuU0HGqF7mnk66rzx0f44goUu42nItTvS6se6MZP920BxkqJW66cBLebW7Fsc5uTBiTgUs55xsREVFq6jfZsnC60qHgcgIfbJQnXJ6sA75skNdNywSu+BFw5Y+BdHkOWcwxAuNnymXo99TK94gp0+Xll61mAjYCTMKSlCRJWLPlQ/zzwDE8/W/zUTBuVKxDoggSBAH3FZ+PU70urN/xOe7e8D4e2PwhHF29nnUm5mTggSWzcMPsiTGMlIiIiKKq/2TLAyvIu0vPuxOwS1YA1/4MyJ7k+zr5c4BbHwVufkSumqjK4j1gIeBwxCT16GtNePqdz/Bl2yl8dKQj1uFQFAiCgAdvno3LCsZClOCVgAFycZbV6214ee+RGEVIREREURfsZMszi4Fb/ug/AetPoQDUo5iAhYhJWBKqafgC//3KAQDAL26aheI57PlIFRKAT1tODvocAKzZsg8ukZMpEhERJb1gJ1uWRODgdk62HEVMwpLMP/cfw083yTdYfv+aQtzJSnkp5d3mVhzt6B70eQnAkfZuvNvcGr2giIiIKDY42XLcYhKWRN77vA13nS5VfptuMsw3zIx1SBRlxzoHT8BGsh4RERElKEkCml4FEOSwQU62HFUszJGgXKLkVflu3vRc/KbuI5zqdeHqc8ejcumFEDhWN+VMGJMR1vWIiIgozoii3MOVlinfnzWY3i5g693wrcbhBydbjjomYQno5b1HsGbLPhxpP9ObMTEnA/9pOBczzhqDnxWfD5WSnZyp6NKCPEzMycDR9u5BT7kTc+Ry9URERJRAju6Ri2zsea5fifilwGV3nSkRf6gRmHjJmeIZ1/4M+LIR2P1X/5Mzu3Gy5ajjN/UE8/LeI1i93uaVgAFy5bt7az/A1TPGYVQ6c+tUpVQIeGDJLACDDz5Yedk0zhdGRESUSPbUApZr5Hm93Pd4uXrkx5ZrgNcfBtYbgXXXAR+9cGa7uXcCtz4C3LaOky3HGX5bTyAuUcKaLfv89nBIkL90r9myD4ZZ+fySncJumD0Rj63Q+fSWZqgU6O4V8dedn+Pb88+GJksdwyiJyGq1ej12OBwwGo0xioaI4lYw83y9ulb+V1ACxw/6vka/yZalPTUQXE5ISjWEOSWcbDlGEjoJczgcqKioAADYbDZYLBZotVqf9ZKloXu3udWnB6y//pXvLi8cG73AKO7cMHsiDLPyve4bPH/iGNz6x7fwaUsX/nPjbqy7Yy4UTNaJYsZkMvksS8S2iYgizD3PV6BbuzTTgZWbgLGF/p8/PdlyX/H/4JWtL+D6m26FSs0LsrGS0EnYwoULsX37dmg0GlitVhgMBjQ1NfmslywNHSvf0XAoFYJPMv7H7+jwzUffxs7mVtiPn8Q5E0bHKDoi0uv1sFgssQ6DiOJZsPN8AUDnESDPtzPCh6CAS5nOIhwxlrBJmNVqRV5eHjQaDQC5MbPb7bDZbNDpdF7rJktDx8p3FKoLJuXg/5ZfjPMnZmP6uFGxDoeIgtTT04OenjNz/XR0dAAAent70dvbG/V43O8Zi/dOBtx/oUmp/ec8CdUw5vnqPdUBqLKGXC2l9l8EhGu/JWwSZrPZfIYearVa2O12nyQsWVxakIf87HQc7fD/xygAyGflOwpg8ZyJsQ6BiADY7XYYDAZYrVZotVpYLBbo9Xq/61ZUVGDNmjU+y7dt24asrKG/cEVSfX19zN47GXD/hSYl9p8k4iYhDUopcE+YS0hD3bZ/Bt3DlRL7LwK6urrC8joJm4S1tLR4esHcNBoNWltbfdYdTkMX7quN4b7aMGPCaL9JmPvP7WeLZ0J09UF0hfY+iXqVhHEPz9tNLXj+/cN46Juzh31/GPd1dKVq3In2eYdDp9OhvLwcGo0GJpMJBoMBbW1tPm0bAJSXl+Oee+7xPO7o6MDUqVOxaNEiZGdnRzFqWW9vL+rr62EwGKBSqaL+/omO+y80KbX/+rohfF4AtH4y5GqSQglhdgmKb7wx4Eum1P6LgJaWlrC8TsImYQ6Hw+/yvDzfXqDhNHSRutoYjqsNkgRk9wgQoEBWGnCy78yX5hy1hNumi3B91oi6z0J+K49EvUrCuAM72Qv80qaEUxTQ2/olDJODmMzRD+7r6Eq1uMN1xTEeVVZWev7fYrGguroaDQ0Nfi8SpqenIz093We5SqWK6ZeoWL9/ouP+C03S77+WJqDmewETMAAQJEC44gdQDGN/JP3+i5Bw7bOETcIKCwuxa9cun+X+EqvhNHThvtoY7qsNNwK413EK+dkZaPisDcc6ezBhTDrmTssNa1n6RL1KwriHacqXuO/5faj7QolvGebi0unBD2Xlvo6uVI3bPRohFWg0GsydOzfWYRBRPNhTC2z5D8DZCWSNBS5ZCbz9B3moYf8iHYo0+So95/lKOAmbhGm1WmzYsMFrmd1uH3SYYX9DNXSRutoYyvZ9LhF9ooQMlRIAMH28/DoLzj1rxPEEK1GvkjDu4Hxr/nQ0fNaOTe8dwj01e/Dij6/CuNG+x/9QuK+jK9XiTsTPGgyz2ex1gdBqtXpGbBBRCpMkoO5eYNc6+fHZVwDGPwPZk+S5vnY8Jidorh5AmS4v4zxfCUkR6wBGymg0wm63w263A5ALdfRPwKqqqmC322E2m722S8SG7g+vHsStf3wLB491xjoUSjKCIOBX35yNcyaMxlcdPbh7w/sQxZENSySi4C1fvhwGgwEmk8nTTpWVlcU4KiKKClEEnCflfwcSBCB7IgABuPpe4Ltb5AQM8MzzhZ8dBe47DNz/lfyYCVhCStieMADYvn07zGYz5s2bh5aWFtTU1Hiec0/c7G7otFotNBoNDAZDQjV0O+wt+MOrn0CUgL2HOnDOhDGxDomSTJY6DY9+R4dbHnkLb3xyHH/850H8aOGMWIdFlNR0Ol3C3t9HRCN0dI888fKe5/r1ZC0FLrsL0EwDMk7f9nLl3UDBN4ApRf5fR6EA1JxmJtEldBKm0+m8Eq/++k/anKgNXdtJJ/7j2fchSsBS3RTcesnkWIdESercs8Zg7a2z8V81u/FZaxckSYLASRyJiIjCY08tsKnU+54uVw/wwQbg/b8DYyYCP2oE1FlykjVYAkZJI6GTsGQmSRLurf0ARzu6oR03Cg/eckGsQ6IkZyyagim5mZhfkMcEjIiIKFyO7pETMMkFDBzx755TqPMwYHsauOz7UQ+PYiNh7wlLds/s+AzWj76CWqnA7791CUalM1+myLtMO9aTgEmSBBfvDyMiIgrNjkcDT6AsKIGjH0QnHooLTMLi0L7DHfjVix8BAH66+DzMnpwT44go1Ti6nFj1dAN+vz3w3CREREQpZajCGv7W3fOcd1l5fySXPGRR4sXPVMHulTikyVLhoik5yM5Q4c4rp8c6HEpBbx48DutHx7B9/zHMm56HBTPGxTokIiKi2BqqsMZgFQqP7pHXDYarB+g9Jd8XRkmPSVgcmqTJxN9XXYZTvS7em0MxcdOFk/DmJ8fx7K4v8B8b3kPdj6/ChOyMWIdFREQUG4MW1tgI7N4gT5Y8xyj3fB1qAPZvBfa/CLQcBBQqQOwN/B7KdECVGdnPQXGDSVgcOX6ixzNRbppSgTFKjhal2PnlzRfg/S8c2H+0Ez/6+3v467/PRxqPSSIiSjVDFtY4nZBtWgXsewH4Yidw4qszzyvVQP6FwJH3hx6SqEiTkzhefE8Z/EYVJz5v6cK1D7+GB7fsg7MviDHGRBGWoVLi0e/oMEqtxM7mVvyvlfeHERFRCgqmsIYkAh9tlhOw9GxgthEwPgnc2wQs+d/A93pJEnDZ6rCFTPGPSVgc6HWJ+NGz76Gzuw+7v3RAwYsgFCe040ejYumFAIA/vnYQ//r46xhHREREFEXBFtYAAEEBfOc5OfEy/hmYfZs8AXP+HHm4oqCUe7z6U6TJy2+rHvy+MkpKHI4YB3677WPs/sKB7Iw0/N/tF3PIF8WVmy+ahJ32Frx24GvkZKpiHQ4REVH09J0KvrCGJALTrgDS1L7PzTEC42cCOx6T7y/zFPYwyj1gTMBSDpOwGHvjk6/xp381AQAql16IKbmsiEPx5+c3zcK917ugyfLTsBARESWrtEw5WQomEQtUWCN/DnDro8DNj8jJnSqL94ClMHa5xNDXnT24e8NuAMC355+NxXMmxjgiIv8yVEqvBOxoezd2Nrei8biAnc2tnNSZiIiSk0IBzLoliPWGUVhDoQDUo5iApTj2hMWIJEn4r5rdOH6iB+eeNRq/uGlWrEMiCsrP/rEHf935+elHSjz9SQMm5mTggSWzcMNsXkggIqIk0t0BfL0/8HosrEHDxJ6wGBEEAcaiKRg/Jh1/+JYOGSplrEMiCujlvUf6JWBnHG3vxur1Nry890gMoiIiIoqQjzYDRz+QhxkKChbWoLBhT1gMLbloEgyzzmICRgnBJUpYs2Wf3+ckAAKANVv2wTArH0qW+CQiomRwyQq57Pw5ejkJY2ENChMmYVHW2d2LU70uTBiTAQBMwChhvNvciiPt3YM+LwE40t6Nd5tbcXnh2OgFRkREFE4dR4D00UD6GPnxVf955jkW1qAw4XDEKJIkCT9/fi+K/+8NvN10PNbhEA3Lsc7BE7CRrEdERBR32j4Fnrge+NvtgLPL/zosrEFhwJ6wCHKJkqeC3NjmVhzucOL59w9DqRCQnsb8lxKLu/c2XOsRERHFleMHgadvBjoOyUMPT7UBak4dRJHBJCxCXt57BGu27Ds9fEuuIOe+XnK3fgaKpuXFMjyiYbu0IA8TczJwtL0b/grSCwDyczJwaQGPbSIiSjBf7QOevgU4eQwYNxO44wUgmxV/KXLYHRMBL+89gtXrbT73z7i/uGrHjY5+UEQhUioEPLBEnkph4AAM9+MHlsxiUQ4iIopvogg4T8r/AsDh94CniuUE7Kw5wJ11TMAo4tgTFmbuCnJDTV279sV9uH42K8hR4rlh9kQ8tkLXr5dXln96nrDrzjsL3b0uFpwhIqL4c3QPsONRYM9zZ6obaq8BPn0L6D0JTC4CVjwHZObGOlJKAUzCwixQBTmAFeQosd0weyIMs/LxzsFj2PbGTiy6aj4uP2cCPv6qE7f88S3Mm56LB2+ZHeswiYiIzthTC2wqlYtpiH3yMlcPcHA7ILmAsTOAlc8DGdkxDZNSB4cjhhkryFEqUCoEzC/IQ9E4CfML8qBUCGg54cRHRzrw9Duf4c1PWP2TiIjixNE9cgImuc4kYG6SS/631Q44Pot+bJSyQkrCNm3ahNWrV+Phhx/2Wv7++++H8rIJjRXkKFUtmDEOKy+bBgAoq92Nju7eGEdExHaKiCAPQQxUTl4Q5ImYiaJkxEnY8uXLYTQaYbFYYLFYvJ7705/+hFdffTXk4BKRu4LcYH/qAoCJrCBHSeqni8/DtLFZONzejbVb9sU6HEpxbKeICKIo3wM2sAfMZ70+eciiNNRd/UThM6Ik7LnnnkNjYyNqampw8OBBLFy40Ov5srIyVFZWhiXARMMKcpTKRqWn4eGSiyAIQE3jl7Du+yrWIVGKYjtFRACAvlPyvV/BcPUAvaciGw/RaSNKwnbt2oWamhosXboUWq0WbW1tXs+PHTsWDQ0NYQkwEbkryOXneA85zM/JwGMrdLhhNsueUvKaNz0Pq67SAgB+umkP2k46YxwRpSK2U0SEYx8Bf1se/PrKdECVGbl4iPoZcXXEwsJCz/9LA7pu2bANXkGOPWCUCu4xnItX9x9DbpYKp3pdYLFfigW2U0RJShKhdPUAkhh43U/fxJmxSEMMNVSkAXOMge8dIwqTEfWEGQwGVFRUeB4L/Q7Yjo4OmEwmzJ07N/ToEpy/CnJEqSBDpcRf/30+ni29HJM0vKpI0cd2iigJHd0D8R+roXxoCm76YBWUD02B+I/VcvVDADi2H2h44sz6E84Hbv498J0aQAjwlVeSgMtWRy52ogFG1BO2cOFCVFZW4vbbb4der4fNZsM//vEPvPvuu6iurobD4UBVVVW4YyWiBHJWtvdwXJco8UIERQ3bKaIks6cW0qZVECUgDXIPmEJ0om/3sxB2Pwth6nzgy50ABKDgGmDs6Z5w3R3yv7dV+84TBsg9YJIkP58/J7qfiVLaiIcj1tTU4LrrrsPGjRsBAEaj0TPco7KyErfddlt4IiSihHbK6ULVK/vxZdspVK8s8uqRIIoktlNESeLoHkibVkESRaQNaELSIMoFDb94R15w3k3+hxTOMQLjZ8pl6PfUykU4lOny8stWMwGjqBtxEpaTk4PGxkZs374ddrsdDocDWq0Wer0eOTk54YyRiBLYF21d+OuOz+F0iXjOdgjGoimxDolSBNspouQgvvOo3AM2yDU8QYDcNzbjBihu/+vgL5Q/B7j1UeDmR+Sqiaos3gNGMTPiJMxt4cKFPqV/AeDTTz/F9OnTQ315Ikpw5541BncbzkXly/uxZvOHuKJwLO8To6hiO0WUwEQR2FPrGYI4GAUAselVeWhhoMRKoQDUo8IXI9EIjHiy5kDMZnOkXpqIEkzp1VpccrYGnT19KKv9wKdSHVEssJ0iSgB9p6AQg5vqRCE6Oc8XJYwR9YS99957+OlPfzro862trbDZbCMOioiSi1Ih4LclF6H492/gzYPHsX7n51h52bRYh0VJjO0UUZJIy4SoUAeViIkKNRSc54sSxIiSMK1Wi/r6egCARqPxes7hcECj0fBKNxF50Y4fDfMN52HNln34zYsf4eoZ4zBtLIeDUGSwnaJk5RIlvNvcimOd3ZgwJgOXJvsUOAoFHIU3I/vjTUgTBh+S2AclFHNKeI8XJYwRDUfMycmBVqtFW1sbWltbvX62bduGuXPnQhSDmECPiFLKdy+fjsu0eUhTCmg+fjLW4VASYztFyejlvUewoPJVfGvdDvzk2ffxrXU7sKDyVby890isQ4uY3V84UPrJpRAAiINcNxElQClIUFzOeb4ocYz4njCLxeK3upRer0dJSQnuuuuukAIjouSjUAj43bKLse3uq/GNmRNiHQ4lObZTlExe3nsEq9fbcKS922v50fZurF5vS8pErPGzNqx4fCcauqfg9zn3AgoF+qD0WqcPSggKBYTb1rHMPCWUESdh/ipNuc2dOxcWi2WkL01ESWySJhMTczhmnyKP7VRycokS3mlqwQvvH8I7TS1wDdY9kkRcooQ1W/bB3yd1L1uzZV9S7Yt3m1txx593orOnD5cW5KH0B2VQmF6H4qLb4RLUEPsEuAQ1FBfdDsH0ujzfV4KQRBFiVxekEfbGh7o9xYeQS9T709DQEImXJaIk88/9x/DXnZ/jsRU6qJQRK9ZK5COV26lEvqfo5b1HsGbLPq/eoIk5GXhgySzcMHtiVGKIxf57t7nVpwesPwnAkfZuvNvcissLx0Y0llAFs//ebjqO//dUA071unDlOWOx7o65yFKnoduhwvGdOeh4cSKE3l5IKhWyT+Vg3PkqZOTH6AMNQ/f+/Wj9y1/Q8WIdJKcTglqN7BuLkffd7yLjvPMivn2ykEQRUnc3hIwMCIrhf3cIdftwGVES1tzcjOrqar/P2e121NbWQqfThRQYESW3ju5e/MeG99F+qhePvdaEHy+cEeuQKImwnfIvHpKYkXIPxxvY1+MejvfYCl3EP0Os9t+xzsETsJGsF4pQktCX9x7BLzfvw9GOM3HmZ2fglzd777/0NCUUAnDNueNhWVmEDJUS7VtfxKGyMvn+L0nuARJ6e+F4YTM6Nm/G5Koq5Nx0Y3g/bBi1b30Rh8vK5MIhLhcAQHI60b55C9pf2IxJAeIPdftwilUSk2xJ7Ih7wiorKwd9TqvVYt26dSN9aSJKAdkZKjx4ywX4ybPv4/fbP8F1503A7Mm+9+8QjVS8t1MOhwMVFRUAAJvNBovFAq1WG7H3i4ckZqQCDccTIA/HM8zKD5gQjDSJiNX+kyQJz+76Iqh1NVkqbN59GPrzJyBL7f8rXqhJ1EiT0Jf3HsH31/tOC3G0oxvfX2/Dn/rtv6Jpuaj5/hUonDAK6WlKdO/fj0NlZZBEccAdYXJCJkrAobIypJ9TGNSX6Wj3pHTv3y8nUP6GD55OqA4PEX+o24dLuJKYkez/ZEpi3UaUhBUUFECj0WD79u0+DYa/m6CJiPy5+aJJeHnvUby09yj+c+NubP7RlUhPG9jEEg1fIrRTCxcuxPbt26HRaGC1WmEwGNDU1BSR9wpnEhOOWIabBIRrON5Ik4hwJ4E7m1vReFzA2OZWXH7OBJ9tupx9niRKEAScnZeJd4Y4NAQA+TkZOHGqDz/++3vIUitx/QX5uOXiSVhwzjiknR7uHWoSNdIk1CVK+OmmPUO+/n/V7MbEnExcNFUDAJg1Kdvz3PGn/iL3gA2yrQKASwJa/vI0Jlf8ZtD3iFVPSutf/hK4dL4goPXppzHpN77xh7r9QJIoQnA6h3VPWTiSmJHuv2RJYgcacR9iZWUlLrnkEuTk5Hj9RJPD4YDZbIbZbIbBYIDdbg9pPSKKLkEQ8KtbZ2PsKDUOfNWJ/7V+EuuQKInEQzs1GKvViry8PM8cZnq9Hna7PWITSA8niYmkkZRY/7ylCz9+9r2gXn//kQ4U/98buLdmN556qxnvNrfiRE+f571HWl2wtvGLsOw/9+df8UQDnv5EiRVPNHh9fvvXJ/DLzR/i0l9vx+4vHJ7tfnjtDDywZBYEyAlXf+7HDyyZBReAs/Oy0OV04R/vHcL3ntyF+b/Zjgde2ItH/3lwxJ8/1MIgO+wtcHT1Dvr6AHCix4Vv/vEtFP/f67jrr41nXl8U0bp5q2cI4mCUkoj2rVsHnf+vfeuLaL5tKdo3b4HklCd+dicRzbctRfvWF4d8/ZFuL4kiOra+6PmyPyiXCx1bX0TXe+9h/4UXeX4+mj0H7f94Pujth5r/sHv/fhwuL4d93qWY8fNfwD7vUhwuL0f3/v1DvrRXEjMwDpcLEEUcLisb8nVC2f/DSUIjsX2kjHg44qpVq4Z8/tNPP8X06dNH+vJBCfYqYjSvNhLR8IwdnY7f3DYHpmcaYflXE645dzw+PNSOz1q7MC0vCysvnw51Got20PDFQzs1GJvN5tNDp9VqYbfb/d6r1tPTg56eHs/jjo4OAEBvby96e4f+cgsARxzBzctn3XcEF+SPQqZ68B5plyhhR9PXaDwuIOeTY7iscHxQvWevfPgVfvTs7kF7Un5rnIPcUWq83dSCvFFq/PuC6QCAsVlKdJwK/BkBoO1kD/Yd6cC+Ix2oOfM9HmfnZuJYZ8+QScR/1eyGdd9RHGnvwZH2U/i/5RfhvPwxAIA3P/k6qPc/4jiJnU0iPjjUjlkTx+D8/GyMyUgL+Pm/v96G8/NH46OjJzzLN7//JWblyxPa549RYcWlUzB+lAq/qtuPox1njoX8nHT8bPF5WDhzHADghvPH4f0v2rH5gyN4cc9RtJx04i/vfDZozO54fv78XmgylDjpdCE9TYH5BXmedX7wt/eDSkLfOXjMazu3tz45Nui2/YkA9h3pxMkel+e4Fru6oBL7gtpe6O1F0403IuuKKzDebPYs7zlwIKieEOX0aUifOdNnlWC3V0w7G8qcHPR8tB+qqVOQPmOGXMUwiL9RQE5KnCe7PEnKcElOJ47+98PImn8pMi66CIpRozzPddbV4avy+wbtyTqr4jcYU1zs93WPP/lkUEnM8aeewllr1/o8Fcr+F10uOUELIglt/8fz6O3ogCD0u1whCOh67bWgk9hxv/zl6e0HF8w5NxgRqY4IyFcgH3vssUi9/JBXEfs3YMGuR0Sxc/0F+bjtksnY9N4hfKt6h9eXlF/XfYRVVxWgvHhWzOKj5BTpdmooLS0tnnbJTaPRoLXVf09KRUUF1qxZ47N827ZtyMrKCvh+9nYBgw/mOuPPb32Gp9/5FMVTRVw3yTdl2d0iYNOnCjic8us9/cn70Kgl3DZdxEVjB78CL0rAGpvy9N+29xcc6fR/76n9wPPchAwJkzr2edb5/kzgqY+VaO/13d79Kho1ML7jAP7tXAGHTgr4sgs4dFKAwyng87ZTAT/7iR4Xam2HPY9fsL4Je54cnSLI/Wf/8H1sbhPw2pEzF47GZUiYnCXhQLswxOfH6QRMwgW5Eq7Ol3Cuqwl1db4XjM2zgKYOAR29QLYKKMw+CddnjagbkGfNUwC6OcD+dgGvHRbwccfQF7O+PuHE7Y/vAgBMHy3h7jlnvrS+8bHSJ25/tr2xE+u3CRijkjB1tITJWUCaAjj4eXD778JcF+ZPADKUHairq5MXiiIKlWlQuoJLxHrtzTicpsIu9/YAtA+uhVIUh/wEEoC9FQ/hq2UlPs+dtbEG2Rh6D0iiiM9v/xYUp7/st37jGhxfvBgQRZyjVHqWD0VMS8PrR49AWf7Tfi8sYfp/PxzU9gDgeOIJOJ54ApIg4GiJEZ1FRVAfPoxpv/8DIEm+n8HlggTg6E/LsfPwYTgnTYLi5EmoW1qQ1uaAqrUV4155BcIQPWzu1+l4YTM+O34cruwc9GVnoy97DLqnTcOETf8IvP8gee3/cXV1yPrkE6iOt0AZbNIjSeiybh/wusEcuafXdTrx8gsvQFKrh1yvq6sryFccWlBJ2OrVw5uBvLW1FVarNaKNW7BXEaN9tXEg9zbhypqjhXFHVyLGHe6YR6vlLwgDT/OiBFheb4bLJcJ8g+8VyuFKxH0NpG7cwW4Xj+3UUBwOh9/leXm+vQgAUF5ejnvuucfzuKOjA1OnTsWiRYuQnZ3td5v+XKKE2t++jq86/PcGAUCWWglNZhoOt/fgmksvRvGF8v09n7acRN2er5CpUuDJdz722b7dKeDJj5X4w+0X4foLzvIsF0UJbV1OHO3owb8+/hoO51AjUOSvSblZKlw7czyu0OZh8UUTva5IzzjdkwR4nyeE0//91W3e7+/WctKJJ9/6DJY3mod4f9n1sybguvPGY1JOJs6fOAY5mSp5uShh5xD7T74nKx0/XH41tnxwBMoPv8K+I5040t6N490CjncH9zXwt8Y5uPmiSUGtG6wlAM774AjuqRn6nixA3v/52Rk4b+IYFBfP9ix/D/vx1DufB9z+G5fPQ+n699B3eliiSilg5lljkDcqDUDgoa4/WFyE686f4LP86Ds70L5l6CGJLkGBMQY9cm5eAiE9HRdddhkAoO/4cXx6MnBPsCCKyPngA2gOHpQnfxYAQE6cxUEujnhtD0BwuQClEurCQsyYOxeXnu5Z+mrHDnQG6s1RKpFz041YfMstPk999dFHQW2fPmcO1GdPxalGG/oOHcK8b34TGbNn46v770enIACDJFICACgUmPPZZzjr3/8dR//rXpx45ZWAn9nndSQJeW++5bVs+uv/wmdrHgx4/5kgStDs2QPdk09AEAQcqa/HycPDnIBcqcS4n5o9SZckyYVtWn/726B6FwW1GjfcckvAnrCWlpbhxTWIoJKwXbt2DXuceqAPEKpgryJG+2rjYOrr60e8bSwx7uhKxLjDEXOfCDyz032V1P9V7j+/9SnO72tCuEYmJuK+BlIv7mCvOMZjOzWUwsJC7Nq1y2f5wPbKLT09Henp6T7LVSoVVCpVwPdTAfjlzRdg9XobBPhLYoDfLbsI11+Qj/1HOzE1LwsqlfwVYdtHx/E/2w8O+truK82/fukADh7vwlsHj+NIezeOdfTA6RreZLK/XHIBbrlkst/nbrp4CtLSlD6FJfIDFJbI16jwjfPOCioJ+96VWr+FPYLZfw8suQAZ6WqUzJuGknnTAACtJ53Yd7gDGxu+wObdhxFIWlpaUL/P4ZqoGRV4JQCPfqfI7+f/+ZLZeGXfMRxt7x4iCc3AxdPG4vvXFOKDQ+344EsHHF292Hu4I+g473vhQ6zTZEF3dq7X8vH/dic6t26FKPkvZiACUAjAWXet9imsIGRmBv3+6O2F1N4+6IWKYMx4602kDfg7HnfnnejcsnXoDSUJ4773Pb+//2C3n/TLBzyfv/err5A2diygUODESy/7HwrYn8uFE3UvYXJFBdLPPhvd+flQTZ6MtEkT0VlXBwTzt6xUIveOO+D6+mv0ff01XA4H1KNGBT28UnI6keZyQZGZiXF33olcoxHqKVNw3FKNjrq6wEnsLTdj/MqVPk/17f8I7Zu3BNw++6YboQ7QCwYgbH+jQSVhc+fOxbp163DJJZcE/cKLFi0acVDBCPYqYrSvNg7U29uL+vp6GAyGiJxYI4VxR1cixh3OmJ98+1NI+HiINeSrkS15s3DnFdNDeq9E3NdA6sbtHo0QSDy2U0PRarXYsGGD1zK73Q69Xh+x97xh9kQ8tkIXMIk5f6J3WzfzrDHQTdXA1q9QxED9C1Ps+rTNs1wQgPGj0zE6PQ3244F7IyZkZwT8DIZZ+cOurnhpQR4m5mQETCIu9XM/U//3Dmb/9Zc3So0FM8ZBqRCCSsImjBn6849UqJ9fqRDwwJJZAZLQWcgbpcZ/XS+PWJAkCV+2ncIHX7bj0dcO4sMgkrGWk734/jONeL3sWmSozgxfzDjvPEyuqsKhsjK4+s0TBsg9YAoBmFxV5beynTI7G4JaHXRPyPTnauXPKEmABEiiC58uWw4E0SsvqNVQ+in+k3HeeZhUVeVTXVAOUCknUIPEP9LtVWfJvcJiV/D3mElOJ6Tuboy/525M+M8z34UPp6UFlcTk3HIz8s1l3q8pisPa/0KG/DeQVVTkWT72//0bOl4cunAKJAl5d9zh96m8734X7S9sHvH2kRJUEmYymVBYWDisFzaZTCMKKFjBXkWM9tXGwYS6faww7uhKxLjDEfOXjp7AK51eL1z7JxH3NZB6cQe7TTy2U0MxGo1YtWoV7HY7tFotbDZbRBMwt5EkMfpZZ+Gksw+2Z98P+Ppzp+Xi2/PPxsScDOTnZGLCmHSolAq4RAkLKl8NKQlyUyqEIcvQD7ZNMElEoGQulklgKMLx+YebhAqCgKl5WZial4Wj7aeCSsLmTM7B6m8UeiVgbjk33Yj0cwrR8pen0b51K4TeXkgqFXJvugljv3vHoAmMoFAg+8bioHtCMmbM8PvewW4/WA+7O/7Wp5+Wqxi6S7TfdCPy7hg8/lC3FzIyhp0EDfwMoSQxw93//vZfLJLYaAgqCRvOlUU3g8Ew7G2GI9iriLG42khEwzMtL7ihvsGuR6knHtupQLZv3w6z2Yx58+ahpaUFNTU1UXnfkSQxwfbQXF44zu9rhysJCsVIerL8iWUSGIpwfP6RJqErL5+OX9d9hEEq2AOQhxM+t/oKr2q4m3cfxrvNLfjp4vMxOj0Nn00Q8HSxgJfOU0LhFOFSK1GsFXDHBAFD3TEcak9IuHpSMs47D5N+8xtM/NWv5MmKMzOHNSx6JNvHQxIUjv0XqyQ2kiJWHdFsNkf0hudAVxGrqqpgNBpjdrWRiIIXbAO98vLpUYuJkl+k26lAdDpd1BKvUMVqOF+4jTSJCNd7J8PnH0kSqk5TYNVVBbC8Pvh9eauuKvBKwLqcfViz+UO0nHTin/u/xm1XH8NfPqmQa0sIIqAWAPRhc9NWbLFvxUNXVaBY67/Eerz1pAgKBYQQ6gwMd/tYJ0Hh2n+xSGIjKaQk7PHHH0dNTY1PkQuHwwG73R7xxm2oq4gWiwVarRZarTZmVxuJKDgjaaCJghHrdipZxHo4XziNJIkIF/fnf+fgMWx7YycWXTUfl58zISU+v3uakXVvNHtdcFMI8DsNSZY6DX/49iX46XN78OXJg3jy4z8AkHynqxJEiBLw0zfKUagpxMw8/31iydiTEqx4SILCuf+incRGyoiTsO9///tYt24dcnJykJeX5ykDb7fbAYxsaMhwDXUVsf9kzIl0tZEoVQ3WQLtdMFkT3YAo4cVDO5VMYjmcL5koFQLmF+Sh5SMJ86OcgMZaefEs/Oei8/DMO5/is9YuTMvLwsrLpw96ge2KwnF4+T+uQslzz+GzHgGCMEiJ9dPV15/Z9wx+teBXg75/svWkDEc8JEGJvP8iYcRJ2MaNG1FaWorHHnsMzz33HJYuXQpAvrpYWlqKjRs3hi1IIkoN/hroti4nHvlnE37+/F7ML8jDWQGqpxG5sZ0Kv3joyaHEpk5T4P9dpQ284mkZKgUO974NQQhQIl0Q8aK9DmuvXBvUfVLJ0JMyXO4kaNwDD+DlzZtxwy23BFWSPdwSdf+F24jH9jgcDlRWVgKQi198+umnAOSqg8uXL8fDDz8clgCJKLW4G+gHb5mN/3eVFj/Rn4s5k3OweHY+RqVH7DZWSkJspyLD3ZNTNC71enIo+rr7utEnBTdpe5/Ui25Xd+AVU5ygUEBSq1O6FyoejDgJ0+l0nnmzLrnkEtTW1nqe02q1qKioCD06Ikp5KqUCG02X46GlF2I0kzAaBrZTRIkvIy0DaUJwU1UohTSkK3ynGSKKRyNOwkwmE8rLyz2P3333Xdx33314/PHHsWzZsrAER0QEAJnqM3PGSJKE9lPBXRWl1MZ2iijxKQQFigsWA1Lgr6wuqQ/fffm72Ht875DriZKIrt4uiFKAIY5EERRUEvb444/7LFu1ahUkScKmTZsAAA899BD+9Kc/obS0FE1NTSgtLQ1vpESU8o62d+OOJ97Fqr80wDVUPXtKOWyniJLXHRfc4Sm+4Y97eZqQhve/fn/QYXYHWg/g/jfvx7z18zD/b/Mxb/083P/m/TjQeiBCkRMNLqixPWazGYWFhbj22mu9lj/00EOe/9dqtWhubobVaoVWq2XVKSIKO2efiMbP2tDldOGJN5ux6urgb+6m5MZ2iih5zcybiYeuqsBP3yg/M0+Ym6SAQgAeuqoC8/Ln4bUvX8MFYy/wPF39QTXGZY6DSlDh/rfvhwABLkku0e4Undhq34ot9i2oWDD4PGNEkRBUEiZJEpYuXYqxY8fCZDKhtLTUM86+v5ycHE/1KSKicDt7bBbuv3EW7vvHHvz3tgO4ZuZ4nHvWmFiHRXGA7RRRcivWFqNQU4hn9j2DF+116JN6kSaocGNhMVbOWumZH6zk3BLPNl93fY0/7f4TesXBh7C7E7LyN4eeZ4wo3IIajqjX69Ha2ooNGzbg4MGDmD59OpYvX45XX3010vEREXn51qVT8Y2Z4+HsE3HPxvfR6+KYfmI7RZQKZubNxK8W/Ao7bn8Hv8j5BXbevgO/WvCrQROnUapR+InuJ0hXBi7WIUDA+n3rwx0y0aCCSsJMJhMAudLUn/70J7S2tmLZsmV46KGHMGPGDDz88MPo6OiIaKBERAAgCAIql16InEwV9h7qwCOvHox1SBQH2E4RpQ6FoIBaCFxiPUuVhZWzVgZVgMMluVDXXAdpsBvPiMIsqCRs4cKFPsuWLl2Kbdu2oaGhAZIk4brrruNVRyKKirOyM/DgLfKY/0f+eRAffOmIbUAUc2yniMif7r7uIYcj9ucUnZxnjKJmxCXq3XJycnDvvfeioaEBy5YtQ0lJCcaOHYu77rorHPEREfl180WTcOOFEzFtbBYUnHCShsB2iih1ZaRlQK1QB7WuWqFGhjIjwhERycIy8+mmTZtgsVhgtVo93bhNTU3heGkiIr8EQcBvvjkH6WkKZKiUgTeglMZ2iig1KQQFFhcsxlb7Vk8RjsFcMuGSgEMcicIlqJ6wTz/91GfZ+++/j9WrV2Ps2LEoKSlBfX09Fi5ciJqaGoiiiFdeeSXcsRIRecnJVHklYCzSkbrYThHRYFbOWgkJge/12nl0J37b8Nughy8ShSKoJKyyshIA0NHRgYcffhgzZsxAUVERLBYLJEnCvffei6amJmzbto2lf4ko6lyihOrXm7D4/97AiZ6+WIdDMcB2iogGMzNvJioWVEAhKKAUvEdOKAUlFIICV066EgDw1IdP4c6X78TRk0djESqlkKCGI9bX1+P666+H1WoFIM/HotfrYTKZ2JgRUcyd6nXhL29/hkOOU/j1ix+h4rY5sQ6JooztFBENxT3P2Pp961HXXAen6IRaoUZxQTFWzFqBmXkzsf2z7fj5Wz/H7q93w7jFiN8s+A2unnJ1rEOnJBVUEma322G326HRaFBaWory8nLk5OREOjYioqCMTk/DwyUX4VvrduDv736ORbPOwrXnTYh1WBRFbKeIKJCZeTOxdsFarLlyDbr7upGZlul1D9jCaQtxbt65uPdf9+LDlg/xg+0/QM2SGpyXd57X64iSiO6+bmSkZUAhhFzjjlJUUEmYRqPB448/jttuuy3S8RARjcjlhWPxb1cW4Im3mmF+7gNsu/tqaLKCq4hFiY/tFBEFSyEokKXK8vvc1DFT8fTip/Hbht+ix9XjlYAdaD2AZ/Y9g5eaX/L0pC0uWIyVs1YOOmE00WCCSt9LS0vZsBFR3Cu7YSYKx4/Csc4e/PyFD2MdDkUR2ykiChe1Uo3y+eX4xeW/8CzbeGAjlm1Zhq32rXCKTgDyvGJb7VuxbOsy1NnrYhUuJaigkrCHHnoo0nEQEYUsQ6XE75ZdDKVCwJbdh7H1g8OxDomihO0UEYWbe6jhvuP7sHbHWogQfcrcuyQXRElE+ZvlONB6IBZhUoLiQFYiSioXTdXgB98ohEopoOWEM9bhEBFRglv/0XoIGHr+MAEC1u9bH6WIKBkwCSOipPOjhTNQ9+Or8N0rpsc6FCIiSmCiJOKVT18JOM+YS3KhrrnOMxk8USBMwogo6aiUCsw4a4znMRtFIiIaie6+bs89YIE4RSe6Xd0RjoiSBZMwIkpqH3zpwDcffRtftHbFOhQiIkowGWkZUCuCq7SrVqiRocyIcESULJiEEVFSq6jbj/e/cOC/anZDFCVAEqF09QCSGOvQiIgozikEBRYXLIZSUA65nlJQorig2GveMaKhMAkjoqT20NI5yFIr0fHpe/ikeiWUD03BTR+sgvKhKRD/sRo4uifWIRIRURxbOWtlwHvCJEiYPW42Op2dUYqKEh2TMCJKatPGjkL1xc3Yqv4ZCo+8CMXpsf0K0Qlx97OQLFcDe2pjHCUREcWrmXkzUbGgAgpB4dMjphSUUAgK/L/Z/w8PvfsQ7njpDhw+welRKDAmYUSU3I7uwZV77oMAEWmC9xDENIiQRBHSplXsESMiokEVa4ux8aaNWKJd4rlHTK1QY4l2CTbetBH6aXrkZuTioOMgvlP3HXx4/MMYR0zxLi3WARARRZL4zqMQJSBtkGH6CgHokwQo3nkMim8+Gt3giIgoYczMm4m1C9ZizZVr0N3Xjcy0TK97wP52499w1/a78EnbJ/jey99D5dWVuO7s62IYMcUz9oQRUfISRWBPLdIwdBGONLiAPTUAS9kTEVEACkGBLFWWTxGO/FH5ePqGp3Hl5CvR7erGf/zzP/DMvmc4TQr5xSSMiJJX3ynPPWCBKEQn0HsqwgEREVEyG60ejUeuewQl55ZAgoSqXVV449AbXuuIkoiu3i6IrNKb0jgckYiSV1omRIU6qERMVKihUGVGISgiIkpmaYo0/Pyyn+PsMWfjE8cnuGryVQCAA60H8My+Z/BS80twik6oFWosLliMlbNWYmbezBhHTdHGJIyIkpdCAcwxom/3s0MOSeyDEoo5JQDndyEiojAQBAHfm/09SJIEQRBQZ69D+ZvlECDAJbkAAE7Ria32rdhi34KKBRUo1hbHOGqKJg5HJKKkprj8LigFQBxiSL4SEhSXr45eUERElBIEQcCB1gMof7McoiR6EjA3l+SCKIkof7McB1oPxChKigUmYUSU3PLnQLhtHQSFAn3wnt9FhNzzJagygPTsWERHRERJ7pl9z0DA0CMtBAhYv299lCKieMAkjIiS3xwjBNPrUFx0O8TT87uICjVw4XJgwiwgIwfoaolxkERElGxEScRLzS/59IAN5JJcqGuuYyXFFMJ7wogoNeTPgeKbj6L3xv9B3dYXcP1Nt0KlVgMdhwGFChg9PtYREhFRkunu64YzyCq9TtGJbpc8/9hQRElEd183MtIyoBDYn5KomIQRUWoRFHAp088U4cie5P288ySgHhX9uIiIKOlkpGVArVAHlYipFWpkKDOw1b4VZ485G3PGzfGai4zVFZML02ciIrfdG4D/nQN89WGsIyEioiSgEBRYXLAYSkE55HpKQYnigmL0uHrw6x2/xnfqvoObn78Z1R9U4/CJw6iz12HZ1mXYat/qSejc1RWXbV2GOntdND4OhRF7woiIAECSgA+ele8N27ASKH0NyGCxDoocq9Xq9djhcMBoNMYoGiKKlJWzVmKLfcuQ60iQsGLWCpzoPYFvTP0Gtn++HZ92fIo/vPcH/OG9Pwy6nftes/I3y1GoKWSPWAJhEkZEBMjDE297HLBcDbQ2AS/8AFj2NOcOo4gxmUw+y5iEESWfmXkzUbGgwmeeMEDuAZMgoWJBhSeBqriqAid7T8L6mRVbmrZg59GdAd/DXV1x7YK1EfscFF4cjkhE5DZqLLDsL3Khjo82AzsejXVElMT0ej2ampq8fogoORVri7Hxpo1Yol0C9ekqvWqFGku0S7Dxpo0+EzWPUo3CLefcgupF1VApVAFfn9UVE09C94Q5HA5UVFQAAGw2GywWC7Rarc96HPJBREGbMhe4/jfAS/cC9b8AJhcBZ18W66iIiCjBzcybibUL1mLNlWvQ3SdXQRQCjLbo7utGr9gb1OsHW12R4kNCJ2ELFy7E9u3bodFoYLVaYTAY/F5J5JAPIhqWS1cBX+wA9j4H1HwP+P5bci8ZURjZ7XYYDAZYrVZotVpYLBbo9Xq/6/b09KCnp8fzuKOjAwDQ29uL3t7gvqCFk/s9Y/HeyYD7LzTJsP9UUKGvry/gekpJOazqikpRGXC/JMP+i6Vw7beETcKsVivy8vKg0WgAyMM67HY7bDYbdDqd17p6vR4WiyUGURJRQhIEYMnv5SqJM4vlyZyJwkyn06G8vBwajQYmkwkGgwFtbW2edq2/iooKrFmzxmf5tm3bkJWVFYVo/auvr4/ZeycD7r/QpMr+u0B5AXaLuyFCHHK9MdIYbK7bDJUQePgikDr7L9y6urrC8joJm4TZbDafoYdarRZ2u90nCSMiGrb00XKFRBWHdVBwqqur0djYOOjzhYWFKCsr8zyurKz0/L/FYkF1dTUaGhr89oaVl5fjnnvu8Tzu6OjA1KlTsWjRImRnB67i2dfXF9RV92D19fXh7bffxhVXXIG0tIT9KhEz3H+hSaX9JwgCCk8U4jvbvgMEuN2rRWrB34W/o2pBFc7OPnvQ9Xp7e1FfXw+DwQCVKriEjc5oaWkJy+sk7JHb0tLic7VQo9GgtbXVZ91YDvlI1C5fxh1diRh3IsYMDDfuNMC9Xl8P0PIJcNbsyAU3hNTY34NvnwhKS0tD2l6j0WDu3Ll+n0tPT0d6errPcpVKNeSXqI6ODhw/ftyrXQsHSZKQn5+PI0eOBLynhXxx/4Um1fZfmpCG3xf9Hj9p/AkA+K2ueOcFd2LTJ5vwseNj3PfOfXj2xmcD7ptA5w/yL1z7LGGTMIfD4Xd5Xl6ez7J4GPKRqF2+jDu6EjHuRIwZGF7c6t4OXGb/HUb1fIXXZj6IU+njIxjZ0FJhf/cXrmEf8cZsNnv1hFmtVk87FS4dHR04dOgQRo8ejXHjxkGlUoXtC6soijhx4gRGjx4NhYKFloeL+y80qbT/JEnCyZMnIX0t4a/X/hXPfvYs6prr4BSdUCvUKC4oxopZKzAzbya+dd638Iu3f4F7iu5JieQ00QlSHNWyHM5QjqqqKuzatQs1NTWe54uKilBZWTloL5ebIAior6/3u56/nrCpU6fi+PHjQQ35GChRu3wZd3QlYtyJGDMwwrj7eqB8ZgkUh20Q8y+C67svAmkZkQ10gJTa3/10dHRg3LhxaG9vH9E5OF7ZbDaYzWZotVpoNBoYDIaAbVd/HR0dyMnJGXK/2O12qFQqTJkyJexfyERRREdHB7Kzs5P+S3AkcP+FJhX336FDh3Dy5EnMmDEDEqSgqyu+3PwyLhh7AaZmT/Us63H2YHPdZtxcfDPS1b497DS0lpaWsLRLcdUTNpyhHFqtFhs2bPBaZrfbg2rEIjHkI5BE7fJl3NGViHEnYszAMONWqeSJmy1XQ3F0NxTWnwNL/jei8Q0eSgrs7wHbJSOdThfRXs3e3l709PRg3LhxvCJOlASys7PR0dEBl8uFtLQ0ZKkCj87ae3wvyt8sR4YyAw9e+SDOHnM2ntn3DF5qfglO0YmHNjyExQWLsXLWSs9E0RQ9CXv5wGg0wm63w263A5CvKvZPwKqqqmC322E2m722i8SQDyJKAZqpwNJ1AASg8Ulg97OxjohoUC6XfM9IsiaxRKnGXYBkOAV2xmWOw+yxs3Gi9wTuee0elGwpwVb7Vk+5e6foxFb7Vizbugx19rqIxE2Di6uesOHavn07zGYz5s2bh5aWFq+hie6Jm5cvXw6DweA15KN/dSoioqCdoweuMQP/egjY8h9A/hzgrAtiHRXRoNgLRpQcRvK3nD8qH0/c8ATWvL0GLzS9AAmSV1EP4EyRj/I3y1GoKWSPWBQldBKm0+m8Eq/++k/anKg3shNRHLqmDPjyXaDpVWDzj4B/3y7PK0ZERBRnVAq5N1whKCBKg88zJkDA+n3rsXbB2miFlvISdjgiEVFMKJTAbY8D5y4Glj7unYCJIuA8Kf9LRBFRW1uLkpISCIKA3NxcmM1mr4rJNpsNJpMJubm5yM3Nhclkgs1mAyDfkmAwGGIUeeoQJRFdvV1DfumPlP7HR2FhIUpKSmAwGFBUVITc3FwUFRVFPaZYEiURLzW/FPB34ZJcqGuuQxzV60t6Cd0TRkQUE6PGAt/ud0/Y0T3AjkeBPc8Brh5AmQ7MWQpcdpc8ZJEoibhECTvtLfjsmAPTJvRivnYclIro9QYbjUYYjUYIgoBly5Z5lfoH5FEyFosFVqsVGo0GFovF85zD4UBDQwMcDkfQ94ZbrVbMnTuX95IH4UDrAa/CD2qFOuqFH/ofH3q93uv3D2BESXgiHwPdfd2ee8ACcYpOdLvkqosUeUzCiIhCsacW2LQKgAC4x9q7eoAPNgK7NwC3VQNzjDENkShcXt57BGu27MOR9m7Psok5GXhgySzcMHtiDCPz5e8Ls/sLerCqq6thMpnQ1tYWxsiSU529DuVvlkOA4LnPyF34YYt9CyoWVKBYWxzjKAGTyTSs9RP9GMhIy4BaoQ4qEVMpVMhQRnf6lVTG4YhERCN1dI+cgEnimQTMTeyTl20qldcjSnAv7z2C1ettXgkYABxt78bq9Ta8vPdIjCKLnLy8PAD+Ezo640DrAZS/WQ5REv0WfhAlEeVvluNA64EYRShzOBzDSsKBxD8GFIICiwsWQykoA66rFJR458g7UYiKACZhREQjt+NRAAGGYQkCsOOxqIRDFKwuZ9+gP929Lp91O7t78cDmD+HvbhH3sjVb9qGzuzfo1422/veK9b+HzH0PkclkgsFg8JraZteuXZ51qqurPdtVV1fDbDbDZDKhqKjIa5v+72O321FSUuL5f5vNBqvVCqvVitdee82zjXuZ1WqN6D4IVldv16A/Pa4en3Wf3PskhADnQgECntr71KCv293XPeT24VBRUQFA3t/u34ube1qjwsJC1NbWepYPPAaef/75oLYd6jhwc/eyuY89972L4bZy1kpIfv96vXW7umGqN2HNO2twsvdkRGKhMzgckYhoJERRvgdsYA+Yz3p98pDFW/7IKooUN2b94pVBn7t25ng8eeelnsdFa604FSCBkgAcae/GFQ+9is5u//MYXTglB5t/uGBE8Q5m48aNXl9q+7PZbNDpdJ7H7qlq+idgdrsdq1at8hpqVlJSAkD+ou5+7f7v4S4E4r7XyOFwoKCgADabzVONubW1FQ6HwzONjs1mQ15eHhwOh+f1n3/+ea94DQZD3FRznv+3+YM+d9Xkq/Co/lHP42s2XINuV+AEyiW5sLV5K7Y2b/X7/AVjL8CzN4V3/sWBx0dDQwMqKysxd+5c1NTUeB0LeXl5mDdvHqqqqjzL/B0DixYtwksvvRRwW2Dw4wCQf9/u+xcBeX7bhQsXRmTY48y8mahYUOEzXBSQe78kSPjl5b/E/tb9+Nv+v6H241q8degtrLliDS6fdLnP64mSiO6+bmSkZUAhsD9npLjniIhGou+UfO9XMFw9QO+pyMZDFAfEKFdWW7ZsGerr6/3+9E/AAHjmCu3PZrPB4XB4fVF3F/rQ6/VYvnw5AKCsrAxlZWVobW1FVVWV131FGo0GlZWVsFqtnsTP/fzy5ctRVlaGxsZGaDQa6HQ6lJeXw+FwYPr06V6xVFZWQq/Xh23fkO/x4d6//o4FjUbjs//9HQPTp08PatuhjgN3j2h5eblnfaPRCIfDEbHe0GJtMTbetBFLtEugVqgBAGqFGku0S7Dxpo345oxvonx+OZ64/glMHj0ZR04ewd8++pvXaxxoPYD737wf89bPw/y/zce89fNw/5v3x3yYaaJiTxgR0UikZcpVEINJxJTpgIrVpih+7Hvw+kGfUwzosW38uR7vNrfie0/uCvi6f/y2DpcW5AX1uvFAr9dDq9WisLAQWq0Wer1+yMIN7uFiWq3W53UAueekf/LX/0u/W2lpKcxmM5566in8/Oc/BwBYLBasW7cuLJ8pHHZ+e+egzykV3vcW/XPZP3H1hqvRK/YGfF2VQoU3lr/hd+LhaPSoBCrKEcp9X0NtO/A4cCda7mGs7uVardarhy3cZubNxNoFa3H/pfdjc91m3FJ8C9Rqtdc68/LnYdPNm/DH9/+I717wXc/yF+0v4r4374v7wiuJhEkYEdFIKBRyGfoPNspDDgddL02ujhiHX0ApdWWpg2/+s9RpuGrGeEzMycDR9m6/d5YIAPJzMnDVjPFRLVcfKo1Gg6amJtTW1qK+vh4bN25EdXU1GhsbfXrSAHl4mfvf/l+6h1O8QaPRYNWqVZ4kzOFwIC8vL64KP2SpsoJed7R6NIoLirHVvtWnKEd/SkGJGwtuxCj1qHCEOCLx0tPY0tICAD7l8wdOtxApCkEBtaD2mwwD8u//3nn3eh4faD2A8jfK/d5X5v6dl79ZjkJNYdSmIkgGHI5IRDRSl90FBBp+JUnAZaujEw9RhCgVAh5YMguAbyka9+MHlsxKqAQMODMc0Wg0wmKxoK2tDRqNBg0NDX7Xnzt3LgD4DBlzJ2fBfskvLS3Fp59+CqvVioqKCq/CHokomMIPEiSsmLUiShEFb+BE39HYdt68eQB8j6N49cf3/xjw9ytAwPp966MUUXJgEkZENFL5c+R5wASl3OPVnyJNXn7LI0DLwdjERxRGN8yeiMdW6JCf4z2PUH5OBh5boYu7ecKCGdZlt9u9vgi7e6WWLVvmtV7/+730er2nOIdbZWUlKisrfYYpDkan0+Giiy5CVVUV7HZ70NvFK3fhB4Wg8CmFrhSUUAgKVCyoiKteEvc+r66uBiAnUUMlw+5jYCTbDmQ0GqHValFSUuKVvNXW1kZ0OOJIiJKItw69FXA9l+RCXXMdpCjfF5rIOByRiCgUc4zA+JlyGfo9tfI9Ysp0efm8VcD2XwL214D2L4ErfhTraIlCcsPsiTDMysdO+3F8dsyBaRM0mK8dF9UesNraWs8wro0bN0Kj0aC8vNwznM9ms8FisXiKbbhLgNvtdk+J8lWrVnm2MZvNsFgsni/W9fX1ntfS6/XQ6XQwGAwoLS2FxWJBfX09TCYTFi5c6On5Kikp8fx/bW2t1/u4tx3oe9/7Hu6+++64qYgYqmJtMQo1hVi/bz3qmuvgFJ1QK9QoLijGilkropaABTo+3HQ6HUpLSz09kXq9HhaLBYWFhZ7fn9Fo9HsMBLMtgCGPg8bGRpjNZk+1TJ1Oh+XLl8fVsFQA6O7rDmqiZ0C+R6zb1Y3MNN4DHQxBYso6pI6ODuTk5KC9vR3Z2dnD3r63txd1dXUoLi6GSqWKQISRwbijKxHjTsSYgQjHLYpy1URV1pl7wP5ZAfzrIfn/F/0auOKHI3rpVN3foZ6Dk1Wg/dLd3Y3m5mYUFBQgIyPDzyuERhRFdHR0IDs7GwoFB9UMl3v/FRUVoampKdbhhJ27hHlmWuag9x2F9PopePyF8296OOdlURIxb/28oBIxtUKNhhUNEfmdx5OWlhaMGzcu5HYpNY5cIqJoUCgA9SjvIhzXlgPXnB6msu1nwNuPxCY2Iooru3fvxr333ht4xQSkEBTIUmUl/ZfxVKAQFFhcsNhnmOlASkGJ4oJi/s6HgUkYEVGkfWNAIvbOH2MbDxHF3P/8z//4HaZIFG+CLbxSkFOAJ/Y+AZc49OTuJGMSRkQUaYIgJ2JXl8mPX7kPeDd+5gQiouhwT8ZbXV2NadOmxTocoqAEU3jlZ/N/hsf3Po7/afwf3PnKnfii44sYRZs4mIQREUWDIADX3icnYuk5wGTfOYiIKLlt3LgRBoMBVqsVa9asiXU4REEr1hZj400bsUS7BGqFPMGzWqHGEu0SbLxpI0rOLcG9c+/FKNUovHfsPSzdshQbD2z0Wy1RlER09XZBlMRof4y4wuqIYeZyudDbe2bW+N7eXqSlpaG7uxsuV+J0zzLu6ErEuBMxZmDouFUqFZTKoce9h8SdiM29E8ieFLn3IaK4VFpaitLSUk9hCaJEMjNvJtYuWIs1V67xW3jlmzO+iUsnXor737wfDV81YO2OtXj1i1fx4BUPYkLWBBxoPYBn9j2Dl5pf8lTPXFywGCtnrYyr6QuihUlYmEiShKNHj6K9vd0r65ckCfn5+fjiiy8S6mZFxh1diRh3IsYMDB23IAjIyclBfn5+5D6TIHgnYF82AodtwKWrIvN+REREYeQuvOLP5NGT8efr/4z1+9bj/2z/h7cOvYWlm5fiJ5f8BGt3roUAAS5JvgDqFJ3Yat+KLfYtqFhQgWJtcTQ/RswxCQuT9vZ2OBwOjB8/HqNGjfJ8gRNFESdOnMDo0aMTqowq446uRIw7EWMGBo9bkiScPHkSX3/9NTIzM6MzV0v7IeCZbwI97YDYB1y2OvLvSUREFEEKQYE7LrgDV06+EuVvlGN69nSs3bnW7/BDd0JW/mY5CjWFKdUjxiQsDCRJwrFjx5CdnY1x48Z5PSeKIpxOJzIyMhLuiyrjjp5EjDsRYwaGjjszMxM9PT04duwYcnJyIt/Dlz1J7gF742Hg5Z/Ky5iIERFREijUFOKvN/4Vv3jrFxAwdHsqQMD6feuxdsHaKEUXe4nzzSmOuVwuuFwuTiRKlASys7M9f9MRJwjAdfcDV/2X/PjlnwI7/hT59yUiIooCpaDEtk+3eXq8BuOSXKhrrvNbyCNZMQkLg76+PgBAWho7FokSnfvv2P13HXGeROw/5ccvm4Gdlui8NxERUQR193XDKTqDWtcpOtHt6o5wRPGDSVgYJVJxAiLyLyZ/x4IAXPfzM4nYS2XAR1u815FEKF09QIqX9CUiosSRkZbhKWkfiFqhRoYyI8IRxQ923RARxQN3IgYAh98DztHL/390D7DjUaTtqcVNLiekD38AzDECl90F5M+JXbxEREQBKAQFFhcsxlb71oBDEq+aclVKdWiwJ4wG9cILL2DZsmUQBAG5ubkwm81wOBye5202G0wmE3Jzc5GbmwuTyQSbzQYAsFqtMBgMMYo8hYgi4Dwp/xtltbW1+N73vgelUonCwkKUlJTAYDCgqKgIubm5KCoqinpMCc+diH17I6DKBPbUApZrgA82QnDJwzkElxP4YKO8fE9tjAOmlCWJQG9XTHpma2trUVJSAkEQeO6JU5IoQuzqghSjtonHR3xZOWslJAS+12vVnNSaqoU9YXHMJUp4t7kVxzq7MWFMBi4tyINSEb0rBLfccgtWrlwJpVKJZcuWobKy0ut5nU4Hi8UCq9UKjUYDi+XMfSwOhwMNDQ1wOBxBl/q2Wq2YO3dudEqDJ7rTvSPY8xzg6gGU6cCcpVHtHTEajVi0aBFyc3Oh1+u9fv8ARpSE8xiAnIgpVfLveFMpILng03aJp+9X21QKjJ/JHjGKntPnHmHPc9C4eiDF6NxjNBohCALPPXGme/9+tP7lL+h4sQ6S0wlBrUb2jcXI++53kXHeeVGJgcdH/JmZNxMVCypQ/ma51zxhgFy4Q4KEe+feiwvGXQAA6BV7sevoLlwx6YpYhRwV7AmLUy/vPYIFla/iW+t24CfPvo9vrduBBZWv4uW9R2Idmg9/JyWj0Yi2tragT1jV1dXsOQtWv94RuHrkZa6euOsdMZlMw1qfx8AAOx6Fb/Y1gCAAOx6LSjhE3j2z8rlH4LmHTmvf+iKab1uK9s1bIDnlnnvJ6UT75i3y8q0vxjhCGY+P2CjWFmPjTRuxRLvEc4+YWqHGEu0SbLxpI1bMWuFZ94k9T8BUb4L5dTMc3Q6f1xIlEV29XX7nHUsk7AmLQy/vPYLV620+X7+Otndj9XobHluhww2zJ8YktkjJy8sD4D+ho34SpHfE4XDAaDQOaxseA/2IotzLGaiBEfvkL763/FFOyIgiheceGkL3/v04XFbmf2j86ek+DpeVIf2cwqj1iPnD4yO2ZubNxNoFa7HmyjXo7utGZlqm33vAesVeKAQF6prrsOPIDtx/2f0wTDPgQOsBPLPvGbzU/BKcohNqhRqLCxZj5ayVCTnJM3vCIqzL2YdTThe6nH0+P929Lp91O7t78cDmD/1e/3YvW7NlHzq7e/2+pr/XPeWMwnxH/fS/V6z/PWTucdomkwkGgwFms9nz3K5duzzrVFdXe7arrq6G2WyGyWRCUVGR1zb938dut6OkpARjx47Fp59+CpvNBqvV6vlx87csppwnz/z0dg143O277lu/R4D5DuUv42//wfu1vF731IDX7QrrRwKAiooKAPL+LikpQW5uruc5u90Os9mMwsJC1NaeuXI+8Bh4/vnng9rW33Hg/n+36upqmEwmmEwmLFq0CLt37w77Zw6rvlNnejkDcfX4/k6JAhns/BDNc08ExPO5x2AweO6bjndiV9fgPz09Puu2/PmJwBeCBAEtTzwx+Ot2R740OY+P+KAQFMhSZQ1ahOOHl/wQ6xevR2FOIVq7W3HPa/fg2y9+G8u2LsNW+1ZPyXun6MRW+1Ys27oMdfa6aH6EsGBPWITN/mX9oM9dO3M8nrzzUs/jorVWnOodOmGSABxp78YVD72Kzm7/8xhdOCUHm3+4wPNY/7t/4a2fXje8wAfYuHGj14mjP5vNBp1O53ms1Wqh0Wi8EjC73Y5Vq1ahra3Ns6ykpASAfDJ0v3b/93AXAnGP53Y4HCgoKIDNZkN9vbxfW1tb4XA4YDabMW/ePNhsNuTm5uLrr7/2vP727du94jUYDJ7tY+43kwDIV0M0A5+bsQj4Ts2Zx1WF8pfzQMQ+4IMN8o8/ky4BSl878/iP84G79wQfsx8Dj4+GhgZUVlZi7ty5qKmp8ToW8vLyMG/ePFRVVXmW+TsGFi1ahJdeeingtoD/48B99dJgMHjuXwSAyspK3HLLLWhtbQ3pM0dUWqZ8n18wiZgyXS7iQTQcp889fkXr3BMGiXTuqaqqwsKFC73awXh1QDd48YpR11yNs/vdZ3XgiiuBYBIolwsdm7egY/MWv09nzJ6Ngtoav8+NFI+PxDVn/BxsXLIRf9r9J/x5z5+x57j/7ynu+8vK3yxHoaYwoXrE2BOWoMQozyi+bNky1NfX+/3pn4ABcpf9wPHTNpsNDofD62ToLvSh1+uxfPlyAEBZWRnKysrQ2tqKqqoqr7HbGo0GlZWVsFqtnsTP/fzy5ctRVlaGXbt2IScnBzqdDuXl5XA4HNBqtV6xVFZWQq/Xh2/nkM/x4d6//o4FjUbjs//9HQPTp08Palt/x0FjYyM0Go2nR7S8vNyz/tKlS9He3h4/vaH+KBRysQNFgOtkijS5XD2HIlKKSqRzj9FohMPhiO9zT5Lh8ZHY1Eo1fqz7Ma6aclXAdQUIWL9vfRSiCh/2hEXY3l8a0NnRiTHZY6BQeOe8igFfnBp/rse7za343pO7Ar7uH7+tw6UFeX6fG/i61nuuGWbU4afX66HValFYWAitVgu9Xj/kzbHuLvmBCZT7JGe1Wr2Sv/4n1o6ODgBAaWkpzGYzqqurUVZWBgCwWCxYt25d+D5YqO47DAAQRREdnZ3IHtPvOBGU3uve+wlQpQVcQcw8r1QDZc3+v5wLA669/GDnCAIfWqAbn0MZWz/Utv2PAwCexsw9jFWj0UCSJEyfPt3rKmZcuuwuYPcgPQpukgRcthpo/AuQVwAUXB2d2CjxnT73+BWtc08ExPO5B5DbtLg/9wCYaWsc/Eml9/Fx7ptv4JMrrvQU4xiKoFZjxjtv+x+GpuDxkSjHRzSJkoh3Dr8TcD2X5EJdcx0evPLBhJlrjElYhGWp09CnViJLneaThPlb96oZ4zExJwNH27v93hcmAMjPycBVM8YHXa4+U60MvFKEaTQaNDU1oba2FvX19di4cSOqq6vR2Njo05MGwDNUrLW11evENpwbZDUaDUpLS2GxWFBWVgaHw4G8vLz4urlWPUr+VxQBlUt+PNhxkj5G7vX4YOOZG+H9UaQBc0qA9NFBxpA1vJiDEC89jS0tLQDgVaJYFEXcd999yM7OjlVYwcmfA9xWLRc7EATv37kiTU7AbquWj4uXyoC+bmD2UmDRr4DsIYaaEQFnzj3BiNS5JwLi+dwDnBkBIsZg/qzhUGQF3y4oR49G9o3FaN+8xVOEw/+KSmTfdCOUo4Zx7IVZohwfdEZ3X7fnHrBAnKIT3S654Eci4HDEOKNUCHhgySwAvvdAux8/sGRWVOcLCwf3cESj0QiLxeIpX9/Q0OB3/blz5wKAT7e8OzkL9kRqMplgt9thtVpRUVHhVdgjIV12l/zleyju3pE4M3Ci72hsO2/ePAC+x1HCmGMETP8CLlwOSSmX9JWUauDC5fLyOUYgIwe4ZKXcy7D3OeCReXIRBVdvjIOnpMJzT2qde4Yp77vfDer4yLvjjugENAw8PuJbRlqGp6R9IGqFGhnKjAhHFD5MwuLQDbMn4rEVOuTneB9I+TkZcVmePpiuc3ci1H+bvLw8LFu2zGu9/vd76fV6T3EOt8rKSlRWVvoMUxyMTqeDTqdDZWUl7HZ70NvFLXfviKD0vV9IkSYvv606ribvde/z6upqAHJDNVQy7D4GRrLtQEajEVqtFiUlJV4N5AsvvJA4Qz7y5wC3Poo+85fYeuE69JkPAbc+euZ3nJkL3PiwXPBgyqWA8wRQ/3PgsSsB+79iGjolEZ57wnLuqa2tTZxzzzBknHceJlVVySM5BgxXhFIJKBSYVFUV0/L0A/H4SAwKQYHFBYuhHDhM2o9rp16bMEMRAQASDam9vV0CILW3tw+6zqlTp6R9+/ZJp06d8nnO5XJJbW1tksvlGvZ797lE6e2Dx6Xn3/tSevvgcanPJQ77NUbK5XJJTz31lLRw4UIJgKTRaKSysjKpra3Ns05jY6NUWloqQS7aKJWWlkqNjY1STU2NpNPpJACS0WiUGhsbpfr6ekmn00l6vV4qLS2VSktLpaamJs9rtbW1ebYpLS31LC8tLZV0Op1UVlYmlZWVSfX19Z7nBr6PxWLxu78tFosEwGvbeDPs4+TIB5L0j9WS9OB4SXogW/73H6vl5VGyYcMG6Rvf+Magx0d/paWlkkajkQBIer1eampqkgBIOp1OqqmpkSRp6GNgqG39HQf9tbW1SaWlpZJWq5W0Wq20dOlS6amnnhp0Xw/19xxLTqdTev755yWn0zn4Si6XJNnWS1Kl9sxx0flV9IL0I6i4hxDMOTgVBdovETuOT597xNPnHjEG556amhpJr9cn3LnHaDR63jOU7wbx7NRHH0mHysulj+ZcKO2beZ700ZwLpUPl5dKpjz4K6/sMtf+S4fjwJ5x/06Gel6Ntf8t+6cK/XCjNfmr2kD8L/r5Aeu+r9yIez/Hjx8PSLgmSFOUyewmmo6MDOTk5aG9vH/Qeku7ubjQ3N6OgoAAZGd69V6IooqOjA9nZ2QHvCYsnyRZ3YWEhmpqaYhjZ0Ea8v0VRLh2tyop6hbxkO0bchvp7jqXe3l7U1dWhuLgYKpVq6JVPOYB//hrIGgd8o9/VWVH0f8+h+zhKywz7zfHDituPYM7B8c5qtcJsNqOmpsarN97hcHjmLbLZbLBYLEH31gfaL5E+jkVXHzpajyE77ywoBvZ8UECJev4MliSKkLq7IWT6n4w3VMm+//wJ5990qOflWKiz16H8zXIIEDxl6QFAKSghQcKErAk4evIoVAoV/n7j3yNaqr6lpQXjxo0LuV1iYQ5KesMdJpBQFIrh3WBPqSFTAxT/t/eyL3YBL/wAWFwJFF4rLzu6B9jxKLDnOXlOMmW6XBr/srvialhZonI4HCgqKoJOp/N7v8jChQuxfft2aDQaWK1WGAyGuL5Y5EVQxOTiDyUGQaGAMIziHkSBFGuLUagpxPp961HXXAen6IRaoUZxQTFWzFqBqWOm4r4374Naqca5uefGOtygMAmjpFdRUYGamvBOAEmUcP71EHD8APDMrcCsW4FpVwAvl3tXXnT1yBXwdm+Q7++ZY4xlxAlPo9F45gUa2BtgtVq9qrXq9XrY7XbPPbFERORtZt5MrF2wFmuuXIPuPrkKYv9z6+++8Tv0iX2eZV29XXC6nNBkaGIU8dBSow+XUo57wsPq6urEL8ZBFA5L/wxcapJ7MPY9L5e1l1y+JcfFPnn5plK5pyxUkgilqweQ4rskd6QMNiWGzWbzOTdptVqvCe2JiMiXQlAgS5Xlc3FLISigPl1JWJRE/OzNn+H2F2/Hx20fe60nSiK6ersgxrhdSviesMHG2vcXyrh7SkwbN27E6tWrYTQa2QtGBJweolgFXLICeOabQNfxodcXBGDHY3IlxpE4PdQxbU8tbnI5IX34A7lnjUMdAcj3FAxM0DQajWcajoF6enrQ09PjeeyelL63txe9vb7TEfT29kKSJIiiGJE5qdy3k7vfg4aH+y80qbj/RFGEJEno7e2FMsT7MN3nDH/njmRx/NRx7G/dj0MnDmFF3Qo8ePmDmDp6Kv66/6945bNXPMMZr592Pb5z3neGNYQxXPstYZOwQGPt+0vocfc0IqWlpfj+978f6zCI4s9Zs4GezsDriX3y0MTrfgGMOWt49/7sqfVMMi2c7mkTXE4OdexnsDLU7gnpB6qoqMCaNWt8lm/btg1Zfu69SUtLQ35+Pk6cOAGnM7iJTkeiszOIY4kGxf0XmlTaf06nE6dOncLrr7+Ovr4hJk0fhvr6+rC8Try6Q3EHNqRtQFNfE+59414AgAIKiJATd6foxIvNL2Jr81YYs4y4SH1RUK/b1dUVlvgSNgkbaqx9fxx3T0TUT98p+d6vYIi9wO9mygUYcqfLP4urAM1U+flTDiAtHVBlntnm6B45AZNc8uQVXq93+ovDplJg/MyE7BGrrq5GY2PjoM8XFhairKws4OsUFhZi165dPssHG75YXl6Oe+65x/O4o6MDU6dOxaJFiwatjvjFF19g9OjREamOKEkSOjs7MWbMmMSalydOcP+FJhX3X3d3NzIzM3H11VeHpTpifX09DAZDwlRHHKlbxVvxwDsP4KXPXgIATwLm5n783KnnsPSapUH1iLW0tIQltoRNwoDBG6v+hhp3zySMiFJOWqZcBTHYRAwC0NsFHNsn/9z8yJmn/lUpV1ccM/FMknZ0L3yzr4EvGeJQxxgqLS0Ny+totVps2LDBa5ndboder/e7fnp6OtLT032Wq1Qqv1+iXC4XBEGAQqGISAlv9xAw93vQ8HD/hSYV959CoYAgCIP+zY9EOF8rXqmggjpN7dUD5o8AAc9+/CzWLlgb+DXDtM8SOgkLRqTH3QNAX18fJEmCy+XyGZucqOOWGXd0JWLciRgzEDhul8sFSZLQ19cXV+PlwzmGX3nBNyHsrfUMFfRHUighzV4GV/FvgfYvITg+AxyfQ1KNAU7HoGw/JFd36jwi/3z+TnABiH2Q9tSgr/h/Aw5zjKffQTgZjUasWrUKdrsdWq0WNptt0AQsFJwKlCg58G95ZERJxEvNLw2ZgAGAS3KhrrkOD175YNR6V5M+CYv0uHtAvhIzceJEtLa2YsyYMX7XSdRxy4w7uhIx7kSMGRg87s7OTpw8eRKvvvpqXDZ64RjDn90zG9eIGwEA/poaCYAkSvhXzwXoeMXa75nxwEsvnXmYfhtUcxZhVM/XyHIew+hTh3H+V88HFYPgcuKVF5+HS+Hbu9NfuMbex4rVavUUBzKZTDCZTDAa5fvhtm/fDrPZjHnz5qGlpSWsRYRUKhUEQcDJkyeRmZkZeAMiimvuc2Gy91yFW3dfN5xicPfFOkUnul1y6ftoiKskLFxj7QduE8lx925fffUVOjo6kJGRgaysM2UzJUnCyZMnMWrUqIQat8y4oysR407EmIHB45YkCV1dXejs7MTEiRNx8cUXxy5IP8I9hl/8cDyUL9wFSQAE0eVZLimUgASItzyKBRcsHd6LSiKkyjq5CEegVZVqXH/jrQF7wtyjERKVXq+HXq+HxWLxeU6n00WseqtSqUROTg6+/vpr9PT0IDs7G2lpaWH7WxVFEU6nE93d3SkzHCycuP9Ck0r7z902HTt2DBqNJuTKiKkmIy0DaoU6qERMrVAjQxn+e2gHE1dJWLjG2vcX6XH3bpMnT4ZSqcTx495lnyVJwqlTp5CZmZlwX1QZd/QkYtyJGDMQOO7c3Fzk5+fH7WcK2xj+i28H8i+Q783aUyvfI6ZMhzDHCFy2GmkjLZoxxyhXQRxiqCMUaRDmlEClVgd8OV71Hbn8/HxkZmbi2LFjYU9mE/XvP15w/4UmFfefRqNBfn5+rMNIOApBgcUFi7HVvhUuyTXoekpBieKC4qgeT3GVhIVTVVUVjEZj1Mbdu4ckTpgwweseht7eXrz++uu4+uqrE+rLBOOOrkSMOxFjBoaOW6VSpdZVxvw5cnGMmx+RqyaqsoZXit6fy+6Sy9APRZKAy1aH9j4UkCAI0Gg0yMnJgcvlCltZayBx//7jBfdfaFJt/6Vc2xRmK2etxBb7liHXkSBhxawVUYpIltBJ2FBj7d0TMmu12oiOux9IqVR6/aEolUr09fUhIyMjoU4UjDu6EjHuRIwZSNy4I0qhANSjwvNa+XPkecBOzxPm1SOmSJMTsNuqE7I8faISBAFpaWlISwtfk8+/o9Bw/4WG+4+GY2beTFQsqED5m+UQIHj1iCkFJSRIqFhQgZl5M6MaV0InYUONte8/GXMkx90TEdEAc4zyPGA7HoO0pwaCywlJqYYwp0TuAWMCRkREUVSsLUahphDr961HXXMdnKITaoUaxQXFWDFrRdQTMCDBkzAiIopTp4c69hX/D17Z+gKuv+nWoO4BIyIiioSZeTOxdsFarLlyDbr75CqIsbynkEkYERFFjqCAS5ke+r1mREREYaAQFMhS+Z92KqpxxDoAIiIiIiKiVMIkjIiIiIiIKIo4HDEASZIAjHzC0N7eXnR1daGjoyOhKvgw7uhKxLgTMWaAcUdbqHG7z73uczHJQm2bQpWox2O84P4LDfdfaLj/QtPZ2Qkg9HaJSVgA7h09derUGEdCRJS6Ojs7kZOTE+sw4gbbJiKi2GppaQmpXRIkXl4ckiiKOHz4MMaMGTOiCiodHR2YOnUqvvjiC2RnZ0cgwshg3NGViHEnYswA4462UOOWJAmdnZ2YNGkSFAqOoHcLtW0KVaIej/GC+y803H+h4f4LTXt7O84++2y0tbVBo9GM+HXYExaAQqHAlClTQn6d7OzshDzQGXd0JWLciRgzwLijLZS42QPmK1xtU6gS9XiMF9x/oeH+Cw33X2hCvTDIy4pERERERERRxCSMiIiIiIgoipiERVh6ejoeeOABpKenxzqUYWHc0ZWIcSdizADjjrZEjZuGxt9raLj/QsP9Fxruv9CEa/+xMAcREREREVEUsSeMiIiIiIgoipiEERERERERRRGTsCixWq0oKiqC3W73Wu5wOGA2m2E2m2EwGHyeJyIiikds14goFUTqXMd5wiLM4XCgqKgIOp0ONpvN5/mFCxdi+/bt0Gg0sFqtMBgMaGpqikGkQ7PZbLBYLNBoNLDZbDCbzdDr9bEOK2g2mw0bNmyAwWBImLjtdjsMBgMaGxtDmgwwWsxmM6qrqwEAer0e69ati7u4HQ4HKioqAJw5prVabYyjGloi7NehJNpxTIElS7sWT6xWq9djh8MBo9EYo2jiWyKex+MJj7XgRfxcJ1HEtbW1SZIkSQCkpqYmz/L6+npJr9d7rQtAamxsjGZ4QdFqtZ7/b2pqkjQaTQyjGR6LxSKVlpbGOoxhKy0tlRLlT7S+vl4yGo1SU1OTVF9fL2k0GsloNMY6LB86nc7z91hfX+91XMejRNmvQ0mk45iClwztWjzRarU+P+Rfop3H4w2PteGJ5LmOwxGjYLCrvzabzefqjVarjbuhGzabDa2trZ7HWq0WeXl5MYwoeLW1taipqYHFYol1KMNSW1uLkpKSWIcRNLvdjnXr1kGr1UKv16OystLnalusWa1W5OXlef4e9Xo97Ha736tb8SIR9utQEu04puAlersWb/R6PZqamrx+yFcinsfjDY+14YnkuY5JWAy1tLT4/HI1Go1XwhMPdDodHA4HSkpK4HA4UFVVhcrKyliHFZRVq1YlTKxuDocDdrs9oYZXlJaWeh3LeXl5mDt3buwC8iMRvxwmwn4dTCIexxS6RGnXKDEl4nmcklM4znVMwmLI4XDA4XD4LI/HXqb6+nrU1tYiNzcX9fX1CXFflbvHoKGhAUVFRcjNzUVVVVWMowqsuroaZWVlsQ4jJBs2bIDJZIp1GF6S4cthPO7XwSTDcUzDl0jtWjxx3zspCAIKCwsTqsc7mpLhPB5rPNbCIxznOhbmGIHq6mo0NjYO+nxhYWFQXz4KCwuxa9cun+XxePN6TU0NampqAMi9S0VFRXHfhW2z2eBwODB37lyUlpZ6hkbp9XrodLpYh+eXzWaL29iC5R4WEm83+vo7WQKJ8+UwXverP8lwHKeaVGzX4olOp0N5eTk0Gg1MJhMMBgPa2tq43wZI9PN4POCxFh7hONcxCRuB0tLSsLyOVqvFhg0bvJbZ7fao9DINp8Gtra0FcObLn16vR25uLmpra6P+hXA4cbe0tHglXEajERqNBg0NDVH9gjicmFetWuVpZNxX9nJzc1FeXh71XoWRfClzV61yJ+zxJJG/HMbzfvUnno5jCk4ytGvxZLjnz/7D5i0WC6qrq9HQ0JBy+y2QRD6Pxwsea+ERlnNdWEqHUFAwoLKKJEmSRqPxLGtsbIzLymdlZWWSxWLxWqbT6aT6+voYRRQci8Ui6XQ6r2VarVaqqamJUUTD09TUlHBV5eLx+HWrqanxOR4SpcpnPO/XQBLxOKbgJWq7Fu80Go2nKhudkcjn8XjFYy04kTjX8Z6wKLBarZ77OEwmk6dnCQC2b98Os9mMqqoqbNiwIS6vdM+bNw/19fWexw6HA3l5eXF/1WTZsmWw2+2eK/LufxNhOFciKikpgclk8lSqstlscXWztNFohN1u98Rks9ni/hgG4n+/UmpK9HYtnpjNZq/HVqvVM1yMvCXqeTxe8Fgbvkie64TT2R3RkKqqqtDU1ITCwkK0tLQkzB+t1WqFxWLBvHnz0NLSApPJlBDV2mpra2GxWGC1WlFaWgqTyRTX99iYTCbPhML9VVZWxtXwM5vNhoqKCs/xEO+VMxNlvw4m0Y5joliw2Wwwm83QarXQaDQwGAxMLIaQaOfxeMJjLb4wCSMiIiIiIooiDkckIiIiIiKKIiZhREREREREUcQkjIiIiIiIKIqYhBEREREREUURkzAiIiIiIqIoYhJGREREREQURUzCiIiIiIiIoohJGBERERERURQxCSMiIiIiIooiJmFEccZut6OkpARFRUXIzc1Fbm4uSkpKYLVaYx0aERGlILZLROHHJIwojtTW1qKoqAjl5eVobGxEW1sbGhsbAQAmk8lnfYfDEeUIiYgolbBdIooMQZIkKdZBEJHccOXm5qKmpgZGo9HnuZKSEtTX13seL1y4EFqtFjU1NbEIl4iIkhzbJaLIYU8YUZxwD+vQ6/U+z2k0Gp8rjjabDXl5eVGJjYiIUg/bJaLISYt1AEQk27VrFwCgoaHBb4M38CokIDeC/litVs/VyXnz5vlsa7fbkZeX59neZrNBq9UGfD2Hw4HCwkLo9XrodLogPxkRESUitktEkcOeMKI4MW/ePABASUkJqqqqYLfbB123uroagNwImUwmlJSUwOFwwOFwwGAwoL6+HuXl5Vi+fDnMZjNKSkoAAGazGbm5uSgsLITVakVtbS0EQfDcbF1UVOTzviaTCRaLBSaTCWazGS0tLVi1alWE9gIREcULtktEESQRUdwoLS2VAHj96HQ6qbS0VGpra5MkSZLq6+ulyspKCYCk1+ulmpoaqaamRpIkSdLpdJLRaPR6zZqaGgmAVFNTI7W1tXltW1lZKbW1tUltbW2SxWKRAEgajcbzXk1NTRIAqampySdOIiJKfmyXiCKDSRhRnGlqapIsFotUWloq6XQ6r4avsbFRkiRJamtrkwBIZWVlnu3q6+slAJLFYvF5PQCeBsq9Xv9t3dwNXmVlpSRJktTY2DjoukRElBrYLhGFH4cjEsUZrVaL0tJSWCwWTzlg99h5i8Uy6Hbu4RqVlZUoKSnx/FRWVkKr1aKwsBAAPDdNu4eZ9Lds2TIAZ+4D0Ol00Ol0qKqq8gwPMZvNLEFMRJRC2C4RhR8LcxDFOY1Gg5qaGgiCgNbW1kHXczdAlZWVfm+WDva9ALnBdWtsbER1dTUsFgtsNhtsNhuqqqrQ2NjIm6CJiFIQ2yWi0LEnjChO2O122Gy2Idfxd5XQzd1Aua8WjjQGAFi+fLlnmcPhQGlpKRobGyFJEiorKwEAGzZsGPH7EBFR/GO7RBQ5TMKI4oTdbvfMyTKQ1WqFRqNBaWnpoNu7ywdXV1cHNSzD39VL99VK95VEm82GjRs3eq1TVlYGYOiGl4iIEh/bJaLIYRJGFCccDgcqKip8Gqva2lqYTCZs377dMyzD/W9tbS2sViuqq6uh0WhQWVkJh8OBoqIiWK1WOBwO2O12mEwmn6uZZrMZtbW1nhLCZrMZAFBTU+NZp7W1FTU1NT7x6HS6EQ8tISKixMB2iShymIQRxQmdTodly5ahsrLSMzeKwWCA3W5HU1OTzzj3yspKtLa2oqSkxDPko6ysDDU1NdBqtSgpKUFBQYFnvpaB22u1WqxatQq5ublYuHAhDAaDzw3WeXl5aG1tRUFBAQwGA0pKSmC329HY2BjZnUFERDHHdokocgRJkqRYB0FE0WOz2VBUVISamhpeNSQiophju0SpiD1hREREREREUcQkjIiIiIiIKIqYhBGlGHf1qaHmdiEiIooWtkuUinhPGFEKMZvNsFqtsNls0Gq10Ov1Pjc9ExERRQvbJUpVTMKIiIiIiIiiiMMRiYiIiIiIoohJGBERERERURQxCSMiIiIiIooiJmFERERERERRxCSMiIiIiIgoipiEERERERERRRGTMCIiIiIioihiEkZERERERBRF/x9/e38kygfVVgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"data/hw1/wave_data_train.pickle\", 'rb') as handle:\n",
    "    wave_data = pickle.load(handle)\n",
    "history = wave_data[\"history\"]\n",
    "future = wave_data[\"future\"]\n",
    "\n",
    "index = 1\n",
    "history_length = history.shape[-1]\n",
    "future_length = future.shape[-1]\n",
    "ts_history = np.arange(-history_length,0)\n",
    "ts_future = np.arange(future_length)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(ts_history, history[index], marker='o', linestyle='--', label=\"History\")\n",
    "plt.plot([-1,0], [history[index][-1], future[index][0]], marker='o', linestyle='--', color=\"C0\")\n",
    "plt.plot(ts_future, future[index], markersize=7, marker='o', linestyle='--', label=\"Future\")\n",
    "\n",
    "plt.xlabel('Steps', fontsize=14)\n",
    "plt.ylabel('Value', fontsize=14)\n",
    "plt.legend(fontsize=12, ncols=2)\n",
    "plt.title(\"Wave data\")\n",
    "plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "with open(\"data/hw1/multimodal_data_test.pickle\", 'rb') as handle:\n",
    "    wave_data = pickle.load(handle)\n",
    "history = wave_data[\"history\"]\n",
    "future = wave_data[\"future\"]\n",
    "\n",
    "\n",
    "index = 1\n",
    "history_length = history.shape[-1]\n",
    "future_length = future.shape[-1]\n",
    "ts_history = np.arange(-history_length,0)\n",
    "ts_future = np.arange(future_length)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ts_history, history[index], marker='o', linestyle='--', label=\"History\")\n",
    "for i in range(3):\n",
    "    plt.plot([-1,0], [history[i*100 + index][-1], future[i*100 + index][0]], marker='o', linestyle='--', color=\"C0\")\n",
    "    plt.plot(ts_future, future[i*100 + index], markersize=7, marker='o', linestyle='--', label=\"Future\")\n",
    "\n",
    "plt.xlabel('Steps', fontsize=14)\n",
    "plt.ylabel('Value', fontsize=14)\n",
    "plt.legend(fontsize=12, ncols=2)\n",
    "plt.title(\"Multimodal data\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with sinusoidal trajectories\n",
    "\n",
    "In this problem, you will learn a regular MLP to regress on sinusoidal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has 10000 examples\n",
      "Test set has 500 examples\n"
     ]
    }
   ],
   "source": [
    "# feel free to poke around the data\n",
    "train_data = hw1_helper.TrajectoryData(\"data/hw1/wave_data_train\")\n",
    "test_data = hw1_helper.TrajectoryData(\"data/hw1/wave_data_test\")\n",
    "\n",
    "history_length = 10\n",
    "future_length = 5\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "print(\"Train set has %i examples\"%len(train_data))\n",
    "print(\"Test set has %i examples\"%len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple MLP predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# Define a simple MLP model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, history_length, future_length, hidden_size=32):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(history_length, hidden_size)\n",
    "        self.f1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, future_length)\n",
    "        \n",
    "        # TODO: construct MLP network\n",
    "        self.model = nn.Sequential(\n",
    "            self.fc1,\n",
    "            self.f1,\n",
    "            self.fc2\n",
    "        )\n",
    "        #############################\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 8\n",
    "history_length = 10\n",
    "future_length = 5\n",
    "\n",
    "model = MLP(history_length, future_length, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [0/313], Loss: 0.4655\n",
      "Epoch [1/30], Step [20/313], Loss: 0.5468\n",
      "Epoch [1/30], Step [40/313], Loss: 0.3783\n",
      "Epoch [1/30], Step [60/313], Loss: 0.3362\n",
      "Epoch [1/30], Step [80/313], Loss: 0.3112\n",
      "Epoch [1/30], Step [100/313], Loss: 0.3244\n",
      "Epoch [1/30], Step [120/313], Loss: 0.2674\n",
      "Epoch [1/30], Step [140/313], Loss: 0.2275\n",
      "Epoch [1/30], Step [160/313], Loss: 0.1989\n",
      "Epoch [1/30], Step [180/313], Loss: 0.1986\n",
      "Epoch [1/30], Step [200/313], Loss: 0.2131\n",
      "Epoch [1/30], Step [220/313], Loss: 0.1600\n",
      "Epoch [1/30], Step [240/313], Loss: 0.1286\n",
      "Epoch [1/30], Step [260/313], Loss: 0.1603\n",
      "Epoch [1/30], Step [280/313], Loss: 0.1017\n",
      "Epoch [1/30], Step [300/313], Loss: 0.1229\n",
      "Epoch 1 completed with average loss: 0.2431\n",
      "Epoch [2/30], Step [0/313], Loss: 0.1318\n",
      "Epoch [2/30], Step [20/313], Loss: 0.1263\n",
      "Epoch [2/30], Step [40/313], Loss: 0.0857\n",
      "Epoch [2/30], Step [60/313], Loss: 0.0815\n",
      "Epoch [2/30], Step [80/313], Loss: 0.1157\n",
      "Epoch [2/30], Step [100/313], Loss: 0.0846\n",
      "Epoch [2/30], Step [120/313], Loss: 0.0945\n",
      "Epoch [2/30], Step [140/313], Loss: 0.0663\n",
      "Epoch [2/30], Step [160/313], Loss: 0.0492\n",
      "Epoch [2/30], Step [180/313], Loss: 0.0474\n",
      "Epoch [2/30], Step [200/313], Loss: 0.0614\n",
      "Epoch [2/30], Step [220/313], Loss: 0.0603\n",
      "Epoch [2/30], Step [240/313], Loss: 0.0676\n",
      "Epoch [2/30], Step [260/313], Loss: 0.0717\n",
      "Epoch [2/30], Step [280/313], Loss: 0.0565\n",
      "Epoch [2/30], Step [300/313], Loss: 0.0496\n",
      "Epoch 2 completed with average loss: 0.0795\n",
      "Epoch [3/30], Step [0/313], Loss: 0.0569\n",
      "Epoch [3/30], Step [20/313], Loss: 0.0499\n",
      "Epoch [3/30], Step [40/313], Loss: 0.0342\n",
      "Epoch [3/30], Step [60/313], Loss: 0.0330\n",
      "Epoch [3/30], Step [80/313], Loss: 0.0293\n",
      "Epoch [3/30], Step [100/313], Loss: 0.0589\n",
      "Epoch [3/30], Step [120/313], Loss: 0.0528\n",
      "Epoch [3/30], Step [140/313], Loss: 0.0506\n",
      "Epoch [3/30], Step [160/313], Loss: 0.0426\n",
      "Epoch [3/30], Step [180/313], Loss: 0.0403\n",
      "Epoch [3/30], Step [200/313], Loss: 0.0422\n",
      "Epoch [3/30], Step [220/313], Loss: 0.0421\n",
      "Epoch [3/30], Step [240/313], Loss: 0.0491\n",
      "Epoch [3/30], Step [260/313], Loss: 0.0381\n",
      "Epoch [3/30], Step [280/313], Loss: 0.0454\n",
      "Epoch [3/30], Step [300/313], Loss: 0.0348\n",
      "Epoch 3 completed with average loss: 0.0427\n",
      "Epoch [4/30], Step [0/313], Loss: 0.0303\n",
      "Epoch [4/30], Step [20/313], Loss: 0.0356\n",
      "Epoch [4/30], Step [40/313], Loss: 0.0408\n",
      "Epoch [4/30], Step [60/313], Loss: 0.0293\n",
      "Epoch [4/30], Step [80/313], Loss: 0.0286\n",
      "Epoch [4/30], Step [100/313], Loss: 0.0286\n",
      "Epoch [4/30], Step [120/313], Loss: 0.0428\n",
      "Epoch [4/30], Step [140/313], Loss: 0.0529\n",
      "Epoch [4/30], Step [160/313], Loss: 0.0296\n",
      "Epoch [4/30], Step [180/313], Loss: 0.0449\n",
      "Epoch [4/30], Step [200/313], Loss: 0.0257\n",
      "Epoch [4/30], Step [220/313], Loss: 0.0255\n",
      "Epoch [4/30], Step [240/313], Loss: 0.0297\n",
      "Epoch [4/30], Step [260/313], Loss: 0.0292\n",
      "Epoch [4/30], Step [280/313], Loss: 0.0284\n",
      "Epoch [4/30], Step [300/313], Loss: 0.0210\n",
      "Epoch 4 completed with average loss: 0.0334\n",
      "Epoch [5/30], Step [0/313], Loss: 0.0336\n",
      "Epoch [5/30], Step [20/313], Loss: 0.0380\n",
      "Epoch [5/30], Step [40/313], Loss: 0.0271\n",
      "Epoch [5/30], Step [60/313], Loss: 0.0443\n",
      "Epoch [5/30], Step [80/313], Loss: 0.0251\n",
      "Epoch [5/30], Step [100/313], Loss: 0.0209\n",
      "Epoch [5/30], Step [120/313], Loss: 0.0341\n",
      "Epoch [5/30], Step [140/313], Loss: 0.0266\n",
      "Epoch [5/30], Step [160/313], Loss: 0.0265\n",
      "Epoch [5/30], Step [180/313], Loss: 0.0427\n",
      "Epoch [5/30], Step [200/313], Loss: 0.0342\n",
      "Epoch [5/30], Step [220/313], Loss: 0.0170\n",
      "Epoch [5/30], Step [240/313], Loss: 0.0276\n",
      "Epoch [5/30], Step [260/313], Loss: 0.0232\n",
      "Epoch [5/30], Step [280/313], Loss: 0.0155\n",
      "Epoch [5/30], Step [300/313], Loss: 0.0309\n",
      "Epoch 5 completed with average loss: 0.0282\n",
      "Epoch [6/30], Step [0/313], Loss: 0.0274\n",
      "Epoch [6/30], Step [20/313], Loss: 0.0239\n",
      "Epoch [6/30], Step [40/313], Loss: 0.0194\n",
      "Epoch [6/30], Step [60/313], Loss: 0.0415\n",
      "Epoch [6/30], Step [80/313], Loss: 0.0284\n",
      "Epoch [6/30], Step [100/313], Loss: 0.0442\n",
      "Epoch [6/30], Step [120/313], Loss: 0.0125\n",
      "Epoch [6/30], Step [140/313], Loss: 0.0107\n",
      "Epoch [6/30], Step [160/313], Loss: 0.0209\n",
      "Epoch [6/30], Step [180/313], Loss: 0.0318\n",
      "Epoch [6/30], Step [200/313], Loss: 0.0199\n",
      "Epoch [6/30], Step [220/313], Loss: 0.0219\n",
      "Epoch [6/30], Step [240/313], Loss: 0.0297\n",
      "Epoch [6/30], Step [260/313], Loss: 0.0181\n",
      "Epoch [6/30], Step [280/313], Loss: 0.0368\n",
      "Epoch [6/30], Step [300/313], Loss: 0.0264\n",
      "Epoch 6 completed with average loss: 0.0259\n",
      "Epoch [7/30], Step [0/313], Loss: 0.0299\n",
      "Epoch [7/30], Step [20/313], Loss: 0.0256\n",
      "Epoch [7/30], Step [40/313], Loss: 0.0370\n",
      "Epoch [7/30], Step [60/313], Loss: 0.0176\n",
      "Epoch [7/30], Step [80/313], Loss: 0.0255\n",
      "Epoch [7/30], Step [100/313], Loss: 0.0348\n",
      "Epoch [7/30], Step [120/313], Loss: 0.0209\n",
      "Epoch [7/30], Step [140/313], Loss: 0.0170\n",
      "Epoch [7/30], Step [160/313], Loss: 0.0234\n",
      "Epoch [7/30], Step [180/313], Loss: 0.0232\n",
      "Epoch [7/30], Step [200/313], Loss: 0.0204\n",
      "Epoch [7/30], Step [220/313], Loss: 0.0226\n",
      "Epoch [7/30], Step [240/313], Loss: 0.0194\n",
      "Epoch [7/30], Step [260/313], Loss: 0.0239\n",
      "Epoch [7/30], Step [280/313], Loss: 0.0181\n",
      "Epoch [7/30], Step [300/313], Loss: 0.0179\n",
      "Epoch 7 completed with average loss: 0.0240\n",
      "Epoch [8/30], Step [0/313], Loss: 0.0162\n",
      "Epoch [8/30], Step [20/313], Loss: 0.0196\n",
      "Epoch [8/30], Step [40/313], Loss: 0.0352\n",
      "Epoch [8/30], Step [60/313], Loss: 0.0241\n",
      "Epoch [8/30], Step [80/313], Loss: 0.0166\n",
      "Epoch [8/30], Step [100/313], Loss: 0.0189\n",
      "Epoch [8/30], Step [120/313], Loss: 0.0239\n",
      "Epoch [8/30], Step [140/313], Loss: 0.0233\n",
      "Epoch [8/30], Step [160/313], Loss: 0.0226\n",
      "Epoch [8/30], Step [180/313], Loss: 0.0128\n",
      "Epoch [8/30], Step [200/313], Loss: 0.0194\n",
      "Epoch [8/30], Step [220/313], Loss: 0.0242\n",
      "Epoch [8/30], Step [240/313], Loss: 0.0205\n",
      "Epoch [8/30], Step [260/313], Loss: 0.0183\n",
      "Epoch [8/30], Step [280/313], Loss: 0.0132\n",
      "Epoch [8/30], Step [300/313], Loss: 0.0216\n",
      "Epoch 8 completed with average loss: 0.0218\n",
      "Epoch [9/30], Step [0/313], Loss: 0.0159\n",
      "Epoch [9/30], Step [20/313], Loss: 0.0154\n",
      "Epoch [9/30], Step [40/313], Loss: 0.0215\n",
      "Epoch [9/30], Step [60/313], Loss: 0.0215\n",
      "Epoch [9/30], Step [80/313], Loss: 0.0155\n",
      "Epoch [9/30], Step [100/313], Loss: 0.0116\n",
      "Epoch [9/30], Step [120/313], Loss: 0.0382\n",
      "Epoch [9/30], Step [140/313], Loss: 0.0198\n",
      "Epoch [9/30], Step [160/313], Loss: 0.0187\n",
      "Epoch [9/30], Step [180/313], Loss: 0.0228\n",
      "Epoch [9/30], Step [200/313], Loss: 0.0209\n",
      "Epoch [9/30], Step [220/313], Loss: 0.0295\n",
      "Epoch [9/30], Step [240/313], Loss: 0.0076\n",
      "Epoch [9/30], Step [260/313], Loss: 0.0123\n",
      "Epoch [9/30], Step [280/313], Loss: 0.0156\n",
      "Epoch [9/30], Step [300/313], Loss: 0.0194\n",
      "Epoch 9 completed with average loss: 0.0194\n",
      "Epoch [10/30], Step [0/313], Loss: 0.0130\n",
      "Epoch [10/30], Step [20/313], Loss: 0.0126\n",
      "Epoch [10/30], Step [40/313], Loss: 0.0206\n",
      "Epoch [10/30], Step [60/313], Loss: 0.0199\n",
      "Epoch [10/30], Step [80/313], Loss: 0.0245\n",
      "Epoch [10/30], Step [100/313], Loss: 0.0236\n",
      "Epoch [10/30], Step [120/313], Loss: 0.0227\n",
      "Epoch [10/30], Step [140/313], Loss: 0.0201\n",
      "Epoch [10/30], Step [160/313], Loss: 0.0094\n",
      "Epoch [10/30], Step [180/313], Loss: 0.0186\n",
      "Epoch [10/30], Step [200/313], Loss: 0.0155\n",
      "Epoch [10/30], Step [220/313], Loss: 0.0169\n",
      "Epoch [10/30], Step [240/313], Loss: 0.0117\n",
      "Epoch [10/30], Step [260/313], Loss: 0.0111\n",
      "Epoch [10/30], Step [280/313], Loss: 0.0137\n",
      "Epoch [10/30], Step [300/313], Loss: 0.0152\n",
      "Epoch 10 completed with average loss: 0.0171\n",
      "Epoch [11/30], Step [0/313], Loss: 0.0202\n",
      "Epoch [11/30], Step [20/313], Loss: 0.0099\n",
      "Epoch [11/30], Step [40/313], Loss: 0.0150\n",
      "Epoch [11/30], Step [60/313], Loss: 0.0203\n",
      "Epoch [11/30], Step [80/313], Loss: 0.0231\n",
      "Epoch [11/30], Step [100/313], Loss: 0.0145\n",
      "Epoch [11/30], Step [120/313], Loss: 0.0228\n",
      "Epoch [11/30], Step [140/313], Loss: 0.0117\n",
      "Epoch [11/30], Step [160/313], Loss: 0.0089\n",
      "Epoch [11/30], Step [180/313], Loss: 0.0090\n",
      "Epoch [11/30], Step [200/313], Loss: 0.0150\n",
      "Epoch [11/30], Step [220/313], Loss: 0.0151\n",
      "Epoch [11/30], Step [240/313], Loss: 0.0117\n",
      "Epoch [11/30], Step [260/313], Loss: 0.0099\n",
      "Epoch [11/30], Step [280/313], Loss: 0.0200\n",
      "Epoch [11/30], Step [300/313], Loss: 0.0106\n",
      "Epoch 11 completed with average loss: 0.0150\n",
      "Epoch [12/30], Step [0/313], Loss: 0.0183\n",
      "Epoch [12/30], Step [20/313], Loss: 0.0150\n",
      "Epoch [12/30], Step [40/313], Loss: 0.0113\n",
      "Epoch [12/30], Step [60/313], Loss: 0.0152\n",
      "Epoch [12/30], Step [80/313], Loss: 0.0131\n",
      "Epoch [12/30], Step [100/313], Loss: 0.0245\n",
      "Epoch [12/30], Step [120/313], Loss: 0.0147\n",
      "Epoch [12/30], Step [140/313], Loss: 0.0136\n",
      "Epoch [12/30], Step [160/313], Loss: 0.0139\n",
      "Epoch [12/30], Step [180/313], Loss: 0.0097\n",
      "Epoch [12/30], Step [200/313], Loss: 0.0087\n",
      "Epoch [12/30], Step [220/313], Loss: 0.0177\n",
      "Epoch [12/30], Step [240/313], Loss: 0.0184\n",
      "Epoch [12/30], Step [260/313], Loss: 0.0163\n",
      "Epoch [12/30], Step [280/313], Loss: 0.0111\n",
      "Epoch [12/30], Step [300/313], Loss: 0.0116\n",
      "Epoch 12 completed with average loss: 0.0131\n",
      "Epoch [13/30], Step [0/313], Loss: 0.0223\n",
      "Epoch [13/30], Step [20/313], Loss: 0.0125\n",
      "Epoch [13/30], Step [40/313], Loss: 0.0131\n",
      "Epoch [13/30], Step [60/313], Loss: 0.0127\n",
      "Epoch [13/30], Step [80/313], Loss: 0.0145\n",
      "Epoch [13/30], Step [100/313], Loss: 0.0086\n",
      "Epoch [13/30], Step [120/313], Loss: 0.0137\n",
      "Epoch [13/30], Step [140/313], Loss: 0.0168\n",
      "Epoch [13/30], Step [160/313], Loss: 0.0145\n",
      "Epoch [13/30], Step [180/313], Loss: 0.0180\n",
      "Epoch [13/30], Step [200/313], Loss: 0.0080\n",
      "Epoch [13/30], Step [220/313], Loss: 0.0168\n",
      "Epoch [13/30], Step [240/313], Loss: 0.0119\n",
      "Epoch [13/30], Step [260/313], Loss: 0.0082\n",
      "Epoch [13/30], Step [280/313], Loss: 0.0080\n",
      "Epoch [13/30], Step [300/313], Loss: 0.0118\n",
      "Epoch 13 completed with average loss: 0.0116\n",
      "Epoch [14/30], Step [0/313], Loss: 0.0176\n",
      "Epoch [14/30], Step [20/313], Loss: 0.0084\n",
      "Epoch [14/30], Step [40/313], Loss: 0.0141\n",
      "Epoch [14/30], Step [60/313], Loss: 0.0158\n",
      "Epoch [14/30], Step [80/313], Loss: 0.0051\n",
      "Epoch [14/30], Step [100/313], Loss: 0.0066\n",
      "Epoch [14/30], Step [120/313], Loss: 0.0117\n",
      "Epoch [14/30], Step [140/313], Loss: 0.0045\n",
      "Epoch [14/30], Step [160/313], Loss: 0.0085\n",
      "Epoch [14/30], Step [180/313], Loss: 0.0097\n",
      "Epoch [14/30], Step [200/313], Loss: 0.0136\n",
      "Epoch [14/30], Step [220/313], Loss: 0.0071\n",
      "Epoch [14/30], Step [240/313], Loss: 0.0188\n",
      "Epoch [14/30], Step [260/313], Loss: 0.0140\n",
      "Epoch [14/30], Step [280/313], Loss: 0.0097\n",
      "Epoch [14/30], Step [300/313], Loss: 0.0060\n",
      "Epoch 14 completed with average loss: 0.0105\n",
      "Epoch [15/30], Step [0/313], Loss: 0.0084\n",
      "Epoch [15/30], Step [20/313], Loss: 0.0132\n",
      "Epoch [15/30], Step [40/313], Loss: 0.0053\n",
      "Epoch [15/30], Step [60/313], Loss: 0.0117\n",
      "Epoch [15/30], Step [80/313], Loss: 0.0048\n",
      "Epoch [15/30], Step [100/313], Loss: 0.0073\n",
      "Epoch [15/30], Step [120/313], Loss: 0.0115\n",
      "Epoch [15/30], Step [140/313], Loss: 0.0104\n",
      "Epoch [15/30], Step [160/313], Loss: 0.0128\n",
      "Epoch [15/30], Step [180/313], Loss: 0.0051\n",
      "Epoch [15/30], Step [200/313], Loss: 0.0059\n",
      "Epoch [15/30], Step [220/313], Loss: 0.0095\n",
      "Epoch [15/30], Step [240/313], Loss: 0.0146\n",
      "Epoch [15/30], Step [260/313], Loss: 0.0186\n",
      "Epoch [15/30], Step [280/313], Loss: 0.0087\n",
      "Epoch [15/30], Step [300/313], Loss: 0.0092\n",
      "Epoch 15 completed with average loss: 0.0095\n",
      "Epoch [16/30], Step [0/313], Loss: 0.0137\n",
      "Epoch [16/30], Step [20/313], Loss: 0.0079\n",
      "Epoch [16/30], Step [40/313], Loss: 0.0080\n",
      "Epoch [16/30], Step [60/313], Loss: 0.0065\n",
      "Epoch [16/30], Step [80/313], Loss: 0.0114\n",
      "Epoch [16/30], Step [100/313], Loss: 0.0094\n",
      "Epoch [16/30], Step [120/313], Loss: 0.0098\n",
      "Epoch [16/30], Step [140/313], Loss: 0.0107\n",
      "Epoch [16/30], Step [160/313], Loss: 0.0068\n",
      "Epoch [16/30], Step [180/313], Loss: 0.0071\n",
      "Epoch [16/30], Step [200/313], Loss: 0.0060\n",
      "Epoch [16/30], Step [220/313], Loss: 0.0047\n",
      "Epoch [16/30], Step [240/313], Loss: 0.0062\n",
      "Epoch [16/30], Step [260/313], Loss: 0.0072\n",
      "Epoch [16/30], Step [280/313], Loss: 0.0109\n",
      "Epoch [16/30], Step [300/313], Loss: 0.0120\n",
      "Epoch 16 completed with average loss: 0.0087\n",
      "Epoch [17/30], Step [0/313], Loss: 0.0037\n",
      "Epoch [17/30], Step [20/313], Loss: 0.0106\n",
      "Epoch [17/30], Step [40/313], Loss: 0.0099\n",
      "Epoch [17/30], Step [60/313], Loss: 0.0071\n",
      "Epoch [17/30], Step [80/313], Loss: 0.0082\n",
      "Epoch [17/30], Step [100/313], Loss: 0.0064\n",
      "Epoch [17/30], Step [120/313], Loss: 0.0056\n",
      "Epoch [17/30], Step [140/313], Loss: 0.0070\n",
      "Epoch [17/30], Step [160/313], Loss: 0.0110\n",
      "Epoch [17/30], Step [180/313], Loss: 0.0035\n",
      "Epoch [17/30], Step [200/313], Loss: 0.0067\n",
      "Epoch [17/30], Step [220/313], Loss: 0.0083\n",
      "Epoch [17/30], Step [240/313], Loss: 0.0099\n",
      "Epoch [17/30], Step [260/313], Loss: 0.0065\n",
      "Epoch [17/30], Step [280/313], Loss: 0.0075\n",
      "Epoch [17/30], Step [300/313], Loss: 0.0057\n",
      "Epoch 17 completed with average loss: 0.0079\n",
      "Epoch [18/30], Step [0/313], Loss: 0.0036\n",
      "Epoch [18/30], Step [20/313], Loss: 0.0072\n",
      "Epoch [18/30], Step [40/313], Loss: 0.0041\n",
      "Epoch [18/30], Step [60/313], Loss: 0.0102\n",
      "Epoch [18/30], Step [80/313], Loss: 0.0065\n",
      "Epoch [18/30], Step [100/313], Loss: 0.0069\n",
      "Epoch [18/30], Step [120/313], Loss: 0.0044\n",
      "Epoch [18/30], Step [140/313], Loss: 0.0041\n",
      "Epoch [18/30], Step [160/313], Loss: 0.0105\n",
      "Epoch [18/30], Step [180/313], Loss: 0.0056\n",
      "Epoch [18/30], Step [200/313], Loss: 0.0058\n",
      "Epoch [18/30], Step [220/313], Loss: 0.0113\n",
      "Epoch [18/30], Step [240/313], Loss: 0.0044\n",
      "Epoch [18/30], Step [260/313], Loss: 0.0089\n",
      "Epoch [18/30], Step [280/313], Loss: 0.0097\n",
      "Epoch [18/30], Step [300/313], Loss: 0.0053\n",
      "Epoch 18 completed with average loss: 0.0072\n",
      "Epoch [19/30], Step [0/313], Loss: 0.0054\n",
      "Epoch [19/30], Step [20/313], Loss: 0.0041\n",
      "Epoch [19/30], Step [40/313], Loss: 0.0055\n",
      "Epoch [19/30], Step [60/313], Loss: 0.0101\n",
      "Epoch [19/30], Step [80/313], Loss: 0.0076\n",
      "Epoch [19/30], Step [100/313], Loss: 0.0057\n",
      "Epoch [19/30], Step [120/313], Loss: 0.0059\n",
      "Epoch [19/30], Step [140/313], Loss: 0.0073\n",
      "Epoch [19/30], Step [160/313], Loss: 0.0039\n",
      "Epoch [19/30], Step [180/313], Loss: 0.0039\n",
      "Epoch [19/30], Step [200/313], Loss: 0.0048\n",
      "Epoch [19/30], Step [220/313], Loss: 0.0110\n",
      "Epoch [19/30], Step [240/313], Loss: 0.0022\n",
      "Epoch [19/30], Step [260/313], Loss: 0.0092\n",
      "Epoch [19/30], Step [280/313], Loss: 0.0065\n",
      "Epoch [19/30], Step [300/313], Loss: 0.0066\n",
      "Epoch 19 completed with average loss: 0.0064\n",
      "Epoch [20/30], Step [0/313], Loss: 0.0039\n",
      "Epoch [20/30], Step [20/313], Loss: 0.0083\n",
      "Epoch [20/30], Step [40/313], Loss: 0.0035\n",
      "Epoch [20/30], Step [60/313], Loss: 0.0077\n",
      "Epoch [20/30], Step [80/313], Loss: 0.0031\n",
      "Epoch [20/30], Step [100/313], Loss: 0.0066\n",
      "Epoch [20/30], Step [120/313], Loss: 0.0080\n",
      "Epoch [20/30], Step [140/313], Loss: 0.0055\n",
      "Epoch [20/30], Step [160/313], Loss: 0.0061\n",
      "Epoch [20/30], Step [180/313], Loss: 0.0047\n",
      "Epoch [20/30], Step [200/313], Loss: 0.0058\n",
      "Epoch [20/30], Step [220/313], Loss: 0.0051\n",
      "Epoch [20/30], Step [240/313], Loss: 0.0060\n",
      "Epoch [20/30], Step [260/313], Loss: 0.0092\n",
      "Epoch [20/30], Step [280/313], Loss: 0.0018\n",
      "Epoch [20/30], Step [300/313], Loss: 0.0017\n",
      "Epoch 20 completed with average loss: 0.0057\n",
      "Epoch [21/30], Step [0/313], Loss: 0.0036\n",
      "Epoch [21/30], Step [20/313], Loss: 0.0033\n",
      "Epoch [21/30], Step [40/313], Loss: 0.0064\n",
      "Epoch [21/30], Step [60/313], Loss: 0.0049\n",
      "Epoch [21/30], Step [80/313], Loss: 0.0039\n",
      "Epoch [21/30], Step [100/313], Loss: 0.0095\n",
      "Epoch [21/30], Step [120/313], Loss: 0.0040\n",
      "Epoch [21/30], Step [140/313], Loss: 0.0062\n",
      "Epoch [21/30], Step [160/313], Loss: 0.0100\n",
      "Epoch [21/30], Step [180/313], Loss: 0.0031\n",
      "Epoch [21/30], Step [200/313], Loss: 0.0041\n",
      "Epoch [21/30], Step [220/313], Loss: 0.0072\n",
      "Epoch [21/30], Step [240/313], Loss: 0.0069\n",
      "Epoch [21/30], Step [260/313], Loss: 0.0020\n",
      "Epoch [21/30], Step [280/313], Loss: 0.0062\n",
      "Epoch [21/30], Step [300/313], Loss: 0.0048\n",
      "Epoch 21 completed with average loss: 0.0050\n",
      "Epoch [22/30], Step [0/313], Loss: 0.0031\n",
      "Epoch [22/30], Step [20/313], Loss: 0.0030\n",
      "Epoch [22/30], Step [40/313], Loss: 0.0077\n",
      "Epoch [22/30], Step [60/313], Loss: 0.0055\n",
      "Epoch [22/30], Step [80/313], Loss: 0.0056\n",
      "Epoch [22/30], Step [100/313], Loss: 0.0053\n",
      "Epoch [22/30], Step [120/313], Loss: 0.0041\n",
      "Epoch [22/30], Step [140/313], Loss: 0.0047\n",
      "Epoch [22/30], Step [160/313], Loss: 0.0046\n",
      "Epoch [22/30], Step [180/313], Loss: 0.0033\n",
      "Epoch [22/30], Step [200/313], Loss: 0.0044\n",
      "Epoch [22/30], Step [220/313], Loss: 0.0020\n",
      "Epoch [22/30], Step [240/313], Loss: 0.0036\n",
      "Epoch [22/30], Step [260/313], Loss: 0.0017\n",
      "Epoch [22/30], Step [280/313], Loss: 0.0014\n",
      "Epoch [22/30], Step [300/313], Loss: 0.0045\n",
      "Epoch 22 completed with average loss: 0.0044\n",
      "Epoch [23/30], Step [0/313], Loss: 0.0072\n",
      "Epoch [23/30], Step [20/313], Loss: 0.0035\n",
      "Epoch [23/30], Step [40/313], Loss: 0.0032\n",
      "Epoch [23/30], Step [60/313], Loss: 0.0058\n",
      "Epoch [23/30], Step [80/313], Loss: 0.0058\n",
      "Epoch [23/30], Step [100/313], Loss: 0.0066\n",
      "Epoch [23/30], Step [120/313], Loss: 0.0014\n",
      "Epoch [23/30], Step [140/313], Loss: 0.0076\n",
      "Epoch [23/30], Step [160/313], Loss: 0.0045\n",
      "Epoch [23/30], Step [180/313], Loss: 0.0049\n",
      "Epoch [23/30], Step [200/313], Loss: 0.0046\n",
      "Epoch [23/30], Step [220/313], Loss: 0.0031\n",
      "Epoch [23/30], Step [240/313], Loss: 0.0021\n",
      "Epoch [23/30], Step [260/313], Loss: 0.0031\n",
      "Epoch [23/30], Step [280/313], Loss: 0.0046\n",
      "Epoch [23/30], Step [300/313], Loss: 0.0051\n",
      "Epoch 23 completed with average loss: 0.0040\n",
      "Epoch [24/30], Step [0/313], Loss: 0.0027\n",
      "Epoch [24/30], Step [20/313], Loss: 0.0041\n",
      "Epoch [24/30], Step [40/313], Loss: 0.0031\n",
      "Epoch [24/30], Step [60/313], Loss: 0.0048\n",
      "Epoch [24/30], Step [80/313], Loss: 0.0024\n",
      "Epoch [24/30], Step [100/313], Loss: 0.0045\n",
      "Epoch [24/30], Step [120/313], Loss: 0.0042\n",
      "Epoch [24/30], Step [140/313], Loss: 0.0052\n",
      "Epoch [24/30], Step [160/313], Loss: 0.0019\n",
      "Epoch [24/30], Step [180/313], Loss: 0.0058\n",
      "Epoch [24/30], Step [200/313], Loss: 0.0037\n",
      "Epoch [24/30], Step [220/313], Loss: 0.0028\n",
      "Epoch [24/30], Step [240/313], Loss: 0.0038\n",
      "Epoch [24/30], Step [260/313], Loss: 0.0024\n",
      "Epoch [24/30], Step [280/313], Loss: 0.0024\n",
      "Epoch [24/30], Step [300/313], Loss: 0.0032\n",
      "Epoch 24 completed with average loss: 0.0036\n",
      "Epoch [25/30], Step [0/313], Loss: 0.0026\n",
      "Epoch [25/30], Step [20/313], Loss: 0.0038\n",
      "Epoch [25/30], Step [40/313], Loss: 0.0016\n",
      "Epoch [25/30], Step [60/313], Loss: 0.0028\n",
      "Epoch [25/30], Step [80/313], Loss: 0.0034\n",
      "Epoch [25/30], Step [100/313], Loss: 0.0031\n",
      "Epoch [25/30], Step [120/313], Loss: 0.0016\n",
      "Epoch [25/30], Step [140/313], Loss: 0.0024\n",
      "Epoch [25/30], Step [160/313], Loss: 0.0022\n",
      "Epoch [25/30], Step [180/313], Loss: 0.0027\n",
      "Epoch [25/30], Step [200/313], Loss: 0.0021\n",
      "Epoch [25/30], Step [220/313], Loss: 0.0027\n",
      "Epoch [25/30], Step [240/313], Loss: 0.0034\n",
      "Epoch [25/30], Step [260/313], Loss: 0.0039\n",
      "Epoch [25/30], Step [280/313], Loss: 0.0048\n",
      "Epoch [25/30], Step [300/313], Loss: 0.0019\n",
      "Epoch 25 completed with average loss: 0.0033\n",
      "Epoch [26/30], Step [0/313], Loss: 0.0045\n",
      "Epoch [26/30], Step [20/313], Loss: 0.0062\n",
      "Epoch [26/30], Step [40/313], Loss: 0.0045\n",
      "Epoch [26/30], Step [60/313], Loss: 0.0028\n",
      "Epoch [26/30], Step [80/313], Loss: 0.0017\n",
      "Epoch [26/30], Step [100/313], Loss: 0.0018\n",
      "Epoch [26/30], Step [120/313], Loss: 0.0036\n",
      "Epoch [26/30], Step [140/313], Loss: 0.0042\n",
      "Epoch [26/30], Step [160/313], Loss: 0.0021\n",
      "Epoch [26/30], Step [180/313], Loss: 0.0033\n",
      "Epoch [26/30], Step [200/313], Loss: 0.0036\n",
      "Epoch [26/30], Step [220/313], Loss: 0.0022\n",
      "Epoch [26/30], Step [240/313], Loss: 0.0031\n",
      "Epoch [26/30], Step [260/313], Loss: 0.0043\n",
      "Epoch [26/30], Step [280/313], Loss: 0.0032\n",
      "Epoch [26/30], Step [300/313], Loss: 0.0034\n",
      "Epoch 26 completed with average loss: 0.0030\n",
      "Epoch [27/30], Step [0/313], Loss: 0.0031\n",
      "Epoch [27/30], Step [20/313], Loss: 0.0032\n",
      "Epoch [27/30], Step [40/313], Loss: 0.0028\n",
      "Epoch [27/30], Step [60/313], Loss: 0.0030\n",
      "Epoch [27/30], Step [80/313], Loss: 0.0035\n",
      "Epoch [27/30], Step [100/313], Loss: 0.0030\n",
      "Epoch [27/30], Step [120/313], Loss: 0.0038\n",
      "Epoch [27/30], Step [140/313], Loss: 0.0039\n",
      "Epoch [27/30], Step [160/313], Loss: 0.0023\n",
      "Epoch [27/30], Step [180/313], Loss: 0.0017\n",
      "Epoch [27/30], Step [200/313], Loss: 0.0031\n",
      "Epoch [27/30], Step [220/313], Loss: 0.0028\n",
      "Epoch [27/30], Step [240/313], Loss: 0.0023\n",
      "Epoch [27/30], Step [260/313], Loss: 0.0023\n",
      "Epoch [27/30], Step [280/313], Loss: 0.0025\n",
      "Epoch [27/30], Step [300/313], Loss: 0.0018\n",
      "Epoch 27 completed with average loss: 0.0028\n",
      "Epoch [28/30], Step [0/313], Loss: 0.0024\n",
      "Epoch [28/30], Step [20/313], Loss: 0.0027\n",
      "Epoch [28/30], Step [40/313], Loss: 0.0023\n",
      "Epoch [28/30], Step [60/313], Loss: 0.0012\n",
      "Epoch [28/30], Step [80/313], Loss: 0.0030\n",
      "Epoch [28/30], Step [100/313], Loss: 0.0017\n",
      "Epoch [28/30], Step [120/313], Loss: 0.0047\n",
      "Epoch [28/30], Step [140/313], Loss: 0.0021\n",
      "Epoch [28/30], Step [160/313], Loss: 0.0040\n",
      "Epoch [28/30], Step [180/313], Loss: 0.0031\n",
      "Epoch [28/30], Step [200/313], Loss: 0.0022\n",
      "Epoch [28/30], Step [220/313], Loss: 0.0036\n",
      "Epoch [28/30], Step [240/313], Loss: 0.0036\n",
      "Epoch [28/30], Step [260/313], Loss: 0.0029\n",
      "Epoch [28/30], Step [280/313], Loss: 0.0022\n",
      "Epoch [28/30], Step [300/313], Loss: 0.0037\n",
      "Epoch 28 completed with average loss: 0.0026\n",
      "Epoch [29/30], Step [0/313], Loss: 0.0029\n",
      "Epoch [29/30], Step [20/313], Loss: 0.0050\n",
      "Epoch [29/30], Step [40/313], Loss: 0.0022\n",
      "Epoch [29/30], Step [60/313], Loss: 0.0026\n",
      "Epoch [29/30], Step [80/313], Loss: 0.0020\n",
      "Epoch [29/30], Step [100/313], Loss: 0.0024\n",
      "Epoch [29/30], Step [120/313], Loss: 0.0029\n",
      "Epoch [29/30], Step [140/313], Loss: 0.0021\n",
      "Epoch [29/30], Step [160/313], Loss: 0.0022\n",
      "Epoch [29/30], Step [180/313], Loss: 0.0025\n",
      "Epoch [29/30], Step [200/313], Loss: 0.0021\n",
      "Epoch [29/30], Step [220/313], Loss: 0.0033\n",
      "Epoch [29/30], Step [240/313], Loss: 0.0028\n",
      "Epoch [29/30], Step [260/313], Loss: 0.0022\n",
      "Epoch [29/30], Step [280/313], Loss: 0.0022\n",
      "Epoch [29/30], Step [300/313], Loss: 0.0033\n",
      "Epoch 29 completed with average loss: 0.0025\n",
      "Epoch [30/30], Step [0/313], Loss: 0.0030\n",
      "Epoch [30/30], Step [20/313], Loss: 0.0021\n",
      "Epoch [30/30], Step [40/313], Loss: 0.0020\n",
      "Epoch [30/30], Step [60/313], Loss: 0.0030\n",
      "Epoch [30/30], Step [80/313], Loss: 0.0021\n",
      "Epoch [30/30], Step [100/313], Loss: 0.0022\n",
      "Epoch [30/30], Step [120/313], Loss: 0.0036\n",
      "Epoch [30/30], Step [140/313], Loss: 0.0037\n",
      "Epoch [30/30], Step [160/313], Loss: 0.0014\n",
      "Epoch [30/30], Step [180/313], Loss: 0.0017\n",
      "Epoch [30/30], Step [200/313], Loss: 0.0028\n",
      "Epoch [30/30], Step [220/313], Loss: 0.0023\n",
      "Epoch [30/30], Step [240/313], Loss: 0.0014\n",
      "Epoch [30/30], Step [260/313], Loss: 0.0021\n",
      "Epoch [30/30], Step [280/313], Loss: 0.0014\n",
      "Epoch [30/30], Step [300/313], Loss: 0.0013\n",
      "Epoch 30 completed with average loss: 0.0023\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "model, optimizer = hw1_helper.train(model, optimizer, train_dataloader, criterion, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c143e09f4181497ab05cc272270507fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Index:', max=499), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function aa598.hw1_helper.plot_data_regression(history, future, prediction, index, xlims=[-11, 5], ylims=[-2, 2])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = model(history)         # Forward pass\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_data)-1, step=1, description='Index:')\n",
    "xlims = [-11, 5]\n",
    "ylims = [-2,2]\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b\n",
    "\n",
    "Benefits of MLPs: simple to implement, relatively few hyperparams\n",
    "\n",
    "Cons of MLPs: uninterpretable, no learning structure. generally only performs well in-distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # HINT: Use Pytorch built-in functions for LSTM and Linear layers.\n",
    "        # HINT: batch dimension is dim=0\n",
    "        \n",
    "        # TODO: Define encoder LSTM.\n",
    "        self.encoder = nn.LSTM(self.input_dim, self.hidden_dim, batch_first=True)\n",
    "        ############################\n",
    "        \n",
    "        # TODO: Define decoder LSTM\n",
    "        # self.decoder = nn.LSTM(self.hidden_dim, self.output_dim, batch_first=True)\n",
    "        self.decoder = nn.LSTM(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
    "        ############################\n",
    "\n",
    "        \n",
    "        #TODO: Define linear project from hidden_dim to output_dim\n",
    "        self.projection = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        ############################\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x, t_max, y=None, prob=1.):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM model.\n",
    "        x: The input sequence [batch_size, seq_len, input_dim]\n",
    "        t_max: maximum time steps to unroll\n",
    "        y: The target sequence for teacher forcing (optional, used if teacher forcing is applied) [batch_size, t_max, output_dim]\n",
    "        prob: Probability to apply teacher forcing (0 to 1). 1 means 100% teacher forcing, \n",
    "        \"\"\"\n",
    "        \n",
    "        # making sure x and y is the appropriate size.\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(-1)\n",
    "        if y is not None and len(y.shape) == 2:\n",
    "            y = y.unsqueeze(-1)\n",
    "        \n",
    "        \n",
    "        ys = [] # collect outputs\n",
    "        # TODO: Run input through encoder to get initial hidden state for decoder\n",
    "\n",
    "        # encoded_x, (h_n, c_n) = self.encoder(x)\n",
    "        encoded_x, _ = self.encoder(x)\n",
    "        ############################\n",
    "\n",
    "        \n",
    "        # TODO: initial state for decoder is last input state\n",
    "        h_n = encoded_x[:, -1, :]\n",
    "        ############################\n",
    "\n",
    "\n",
    "        # TODO: unroll decoder \n",
    "        # TODO: if eval or no teacher forcing, use prediction from previous step\n",
    "        # TODO: if train and using teacher forcing, use prob to determine whether to use ground truth or previous prediction\n",
    "        dec_t = h_n\n",
    "        for t in range(t_max):\n",
    "            if self.training: # and self.teacher_forcing?\n",
    "                if np.random.rand() < prob:\n",
    "                    input,_ = self.encoder(y[:, t, :])\n",
    "                else:\n",
    "                    input = dec_t\n",
    "            else:\n",
    "                input = dec_t\n",
    "            dec_t, (h_t, c_t) = self.decoder(input)\n",
    "            ys.append(self.projection(dec_t))\n",
    "        ############################\n",
    "\n",
    "        ys = torch.stack(ys)\n",
    "        ys = ys.transpose(0, 1)\n",
    "        return ys # [batch_size, ts_max, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 4\n",
    "future_length = 5\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "model = LSTM(input_size, output_size, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "prob = 0.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "def prob_schedule(i):\n",
    "    return 1 - jax.nn.sigmoid(20 * (i - 0.5)).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [0/313], Loss: 0.1117\n",
      "Epoch [1/10], Step [100/313], Loss: 0.0071\n",
      "Epoch [1/10], Step [200/313], Loss: 0.0038\n",
      "Epoch [1/10], Step [300/313], Loss: 0.0025\n",
      "Epoch 1 completed with average loss: 0.0114\n",
      "Epoch [2/10], Step [0/313], Loss: 0.0039\n",
      "Epoch [2/10], Step [100/313], Loss: 0.0023\n",
      "Epoch [2/10], Step [200/313], Loss: 0.0017\n",
      "Epoch [2/10], Step [300/313], Loss: 0.0016\n",
      "Epoch 2 completed with average loss: 0.0027\n",
      "Epoch [3/10], Step [0/313], Loss: 0.0019\n",
      "Epoch [3/10], Step [100/313], Loss: 0.0026\n",
      "Epoch [3/10], Step [200/313], Loss: 0.0024\n",
      "Epoch [3/10], Step [300/313], Loss: 0.0024\n",
      "Epoch 3 completed with average loss: 0.0028\n",
      "Epoch [4/10], Step [0/313], Loss: 0.0018\n",
      "Epoch [4/10], Step [100/313], Loss: 0.0017\n",
      "Epoch [4/10], Step [200/313], Loss: 0.0243\n",
      "Epoch [4/10], Step [300/313], Loss: 0.0200\n",
      "Epoch 4 completed with average loss: 0.0055\n",
      "Epoch [5/10], Step [0/313], Loss: 0.0018\n",
      "Epoch [5/10], Step [100/313], Loss: 0.0099\n",
      "Epoch [5/10], Step [200/313], Loss: 0.0070\n",
      "Epoch [5/10], Step [300/313], Loss: 0.0524\n",
      "Epoch 5 completed with average loss: 0.0268\n",
      "Epoch [6/10], Step [0/313], Loss: 0.0243\n",
      "Epoch [6/10], Step [100/313], Loss: 0.0449\n",
      "Epoch [6/10], Step [200/313], Loss: 0.0439\n",
      "Epoch [6/10], Step [300/313], Loss: 0.0418\n",
      "Epoch 6 completed with average loss: 0.0538\n",
      "Epoch [7/10], Step [0/313], Loss: 0.0587\n",
      "Epoch [7/10], Step [100/313], Loss: 0.0246\n",
      "Epoch [7/10], Step [200/313], Loss: 0.0212\n",
      "Epoch [7/10], Step [300/313], Loss: 0.0144\n",
      "Epoch 7 completed with average loss: 0.0323\n",
      "Epoch [8/10], Step [0/313], Loss: 0.0157\n",
      "Epoch [8/10], Step [100/313], Loss: 0.0197\n",
      "Epoch [8/10], Step [200/313], Loss: 0.0183\n",
      "Epoch [8/10], Step [300/313], Loss: 0.0140\n",
      "Epoch 8 completed with average loss: 0.0192\n",
      "Epoch [9/10], Step [0/313], Loss: 0.0105\n",
      "Epoch [9/10], Step [100/313], Loss: 0.0156\n",
      "Epoch [9/10], Step [200/313], Loss: 0.0119\n",
      "Epoch [9/10], Step [300/313], Loss: 0.0373\n",
      "Epoch 9 completed with average loss: 0.0161\n",
      "Epoch [10/10], Step [0/313], Loss: 0.0108\n",
      "Epoch [10/10], Step [100/313], Loss: 0.0130\n",
      "Epoch [10/10], Step [200/313], Loss: 0.0170\n",
      "Epoch [10/10], Step [300/313], Loss: 0.0105\n",
      "Epoch 10 completed with average loss: 0.0144\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# we use a slightly different training loop to account for teacher forcing\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    prob = prob_schedule((epoch + 1)/num_epochs)\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()        # Zero the gradients\n",
    "        target = target.unsqueeze(-1)\n",
    "        output = model(data, future_length, target, prob)         # Forward pass\n",
    "        loss = criterion(output, target)  # Compute loss\n",
    "        loss.backward()              # Backpropagation\n",
    "        optimizer.step()             # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch+1} completed with average loss: {running_loss/len(train_dataloader):.4f}')\n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/than/Documents/Classes/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([500, 5])) that is different to the input size (torch.Size([500, 5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (500) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (history, future) \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[1;32m      5\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model(history, future_length)         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print out test loss\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Classes/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Classes/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/Classes/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:608\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Classes/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/nn/functional.py:3791\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[1;32m   3793\u001b[0m     expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3794\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Classes/AA598-aut24/aa598_venv/lib/python3.12/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (500) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = model(history, future_length)         # Forward pass\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# try with different prediction horizons\n",
    "prediction_horizon = 20\n",
    "prediction = model(history, prediction_horizon)\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "test_dataloader = list(DataLoader(test_data, batch_size=1, shuffle=False))\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_dataloader)-1, step=1, description='Index:')\n",
    "xlims = [-11, prediction_horizon + 2]\n",
    "ylims = [-5,5]\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on multimodal data\n",
    "\n",
    "Now we repeat the same steps but with data where the future has multimodal outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has 15000 examples\n",
      "Test set has 300 examples\n"
     ]
    }
   ],
   "source": [
    "# load multimodal data\n",
    "train_data = hw1_helper.TrajectoryData(\"data/hw1/multimodal_data_train\")\n",
    "test_data = hw1_helper.TrajectoryData(\"data/hw1/multimodal_data_test\")\n",
    "\n",
    "history_length = 11\n",
    "future_length = 10\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "print(\"Train set has %i examples\"%len(train_data))\n",
    "print(\"Test set has %i examples\"%len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple MLP predictor (multimodal data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_size = 32\n",
    "# You should be able to use your MLP class\n",
    "model = MLP(history_length, future_length, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = hw1_helper.train(model, optimizer, train_dataloader, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = model(history)         # Forward pass\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "test_dataloader = list(DataLoader(test_data, batch_size=1, shuffle=False))\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_dataloader)-1, step=1, description='Index:')\n",
    "xlims = [-11, 10]\n",
    "ylims = [-12, 12]\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM predictor (multimodal data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 4\n",
    "# You should be able to use your LSTM class\n",
    "model = LSTM(input_size, output_size, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a slightly different training loop to account for teacher forcing\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()        # Zero the gradients\n",
    "        output = model(data, future_length, target, prob)         # Forward pass\n",
    "        loss = criterion(output, target)  # Compute loss\n",
    "        loss.backward()              # Backpropagation\n",
    "        optimizer.step()             # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch+1} completed with average loss: {running_loss/len(train_dataloader):.4f}')\n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = model(history, future_length)         # Forward pass\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# TODO: try with different prediction horizons\n",
    "prediction_horizon = future_length\n",
    "prediction = model(history, prediction_horizon)\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "test_dataloader = list(DataLoader(test_data, batch_size=1, shuffle=False))\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_dataloader)-1, step=1, description='Index:')\n",
    "\n",
    "xlims = [-11, prediction_horizon + 2]\n",
    "ylims = [-12, 12]\n",
    "\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider CVAEs\n",
    "\n",
    "First, define the encoder and decoder. We will consider some simple MLP encoders. Generally, for trajectory data, it's typically more common to use RNNs or transformers, but since we are considering a small toy problem, we just consider MLP for now since it's simpler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# some simple MLP encoders. For trajectory data, it's typically more common to use RNNs or transformers\n",
    "class MLPEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=32):\n",
    "        super(MLPEncoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # TODO: Construct an MLP encoder\n",
    "        self.model = None   \n",
    "        ############################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "class MLPDecoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=32):\n",
    "        super(MLPDecoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # TODO: Construct an MLP encoder\n",
    "        self.model = None\n",
    "        ############################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Some helper functions\n",
    "def repeat_n(ten, n):\n",
    "    return torch.stack([ten] * n, dim=0)\n",
    "\n",
    "def beta_schedule(i):\n",
    "    return jax.nn.sigmoid(20 * (i - 0.5)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = hw1_helper.TrajectoryData(\"data/hw1/multimodal_data_train\")\n",
    "test_data = hw1_helper.TrajectoryData(\"data/hw1/multimodal_data_test\")\n",
    "\n",
    "history_length = 11\n",
    "future_length = 10\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "print(\"Train set has %i examples\"%len(train_data))\n",
    "print(\"Test set has %i examples\"%len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousCVAE(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, importance, decoder, prior):\n",
    "        '''\n",
    "        latent_dim: dimension of the continuous latent space\n",
    "        importance: network to encode the importance weight\n",
    "        decoder: network to decode the output\n",
    "        prior: network to encode the prior        \n",
    "        '''\n",
    "        \n",
    "        super(ContinuousCVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.importance = importance\n",
    "        self.decoder = decoder\n",
    "        self.prior = prior\n",
    "        \n",
    "        # TODO: Linear layers to project encoder/decoder to mean and logvar\n",
    "        self.mean_projection_encoder = None\n",
    "        self.logvar_projection_encoder = None\n",
    "        self.mean_projection_decoder = None\n",
    "        self.logvar_projection_decoder = None\n",
    "        ############################\n",
    "\n",
    "        \n",
    "    def encode_importance(self, x, y):\n",
    "        '''Computes mean and log(covariance) of q(z|x,y), assumes normal distribution'''\n",
    "        \n",
    "        # TODO: compute z_mu and z_logvar of q(z|x,y)\n",
    "        z_mu = None\n",
    "        z_logvar = None\n",
    "        ############################\n",
    "        \n",
    "        return z_mu, z_logvar\n",
    "    \n",
    "    \n",
    "    def encode_prior(self, x):\n",
    "        '''Computes mean and log(covariance) of p(z|x), assumes normal distribution'''\n",
    "        \n",
    "        # TODO: compute z_mu and z_logvar of p(z|x)\n",
    "        z_mu = None\n",
    "        z_logvar = None\n",
    "        ############################\n",
    "        \n",
    "        return z_mu, z_logvar\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, logvar, n=1):\n",
    "        '''samples from a normal distributions parameterized by mu and logvar. Uses PyTorch built-in reparameratization trick'''\n",
    "        \n",
    "        prob = torch.distributions.MultivariateNormal(loc=mu, covariance_matrix=torch.diag_embed(torch.exp(logvar)))\n",
    "        \n",
    "        return prob.rsample((n,))\n",
    "    \n",
    "    \n",
    "    def decode(self, x, z):\n",
    "        '''Computes mean and log(covariance) of p(y|x,z), assumes normal distribution'''\n",
    "        \n",
    "        # TODO: compute y_mu and y_logvar of p(y|x,z)\n",
    "        y_mu = None\n",
    "        y_logvar = None\n",
    "        ############################\n",
    "        \n",
    "        return y_mu, y_logvar\n",
    "\n",
    "    \n",
    "    def forward(self, x, y, n=1):\n",
    "        '''forward pass of the cvae model'''\n",
    "        \n",
    "        #  get p(z|x,(y))\n",
    "        if self.training:\n",
    "            z_mu, z_logvar = self.encode_importance(x, y)\n",
    "        else:\n",
    "            z_mu, z_logvar = self.encode_prior(x)\n",
    "        # sample from p(z|x,(y)) n times\n",
    "        z = self.reparameterize(z_mu, z_logvar, n)\n",
    "        # get p(y|x,z)\n",
    "        y_mu, y_logvar  = self.decode(repeat_n(x, n), z)     \n",
    "           \n",
    "        return z_mu, z_logvar, y_mu, y_logvar\n",
    "    \n",
    "\n",
    "    \n",
    "    def sample(self, x, num_samples=8, num_latent_samples=8):\n",
    "        '''samples from p(y|x,z) where z~p(z|x). Need to specify the number z and y samples to draw'''\n",
    "        \n",
    "        _, _, y_mu, y_logvar = self.forward(x, None, num_latent_samples)\n",
    "\n",
    "        return self.reparameterize(y_mu, y_logvar, num_samples)\n",
    "\n",
    "    \n",
    "    \n",
    "    def elbo(self, x, y, z_samples=1, beta=1.):\n",
    "        '''Compute ELBO for CVAE with continuous latent space. Optional: beta term that weigh kl divergence term'''\n",
    "        \n",
    "        q_mu, q_logvar, y_mu, y_logvar = self(x, y, z_samples) # get parameters for q(z|x,y) and p(y|x,z) where z~q(z|x,y)\n",
    "        p_mu, p_logvar = self.encode_prior(x) # get parameters for p(z|x)\n",
    "        \n",
    "        # construct the distributions\n",
    "        y_prob = torch.distributions.MultivariateNormal(loc=y_mu, covariance_matrix=torch.diag_embed(torch.exp(y_logvar)))  # p(y|x, z)\n",
    "        q = torch.distributions.MultivariateNormal(loc=q_mu, covariance_matrix=torch.diag_embed(torch.exp(q_logvar)))  # q(z|x,y)\n",
    "        p = torch.distributions.MultivariateNormal(loc=p_mu, covariance_matrix=torch.diag_embed(torch.exp(p_logvar)))  # p(z|x)\n",
    "        \n",
    "        loglikelihood = -y_prob.log_prob(repeat_n(y, z_samples)).mean() # log likelihood of data \n",
    "        kl_div = torch.distributions.kl.kl_divergence(q, p).mean()  # q_z * (log(q_z) - log(p_z))\n",
    "        \n",
    "        return loglikelihood + beta * kl_div\n",
    "        \n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous cvae\n",
    "# network parameters\n",
    "latent_dim = 1 # size of latent space\n",
    "history_length = 11\n",
    "future_length = 10\n",
    "hidden_dim = 8\n",
    "enc_dim = 8\n",
    "dec_dim = 8\n",
    "\n",
    "encoder = MLPEncoder(history_length + future_length, hidden_dim, enc_dim)\n",
    "prior = MLPEncoder(history_length, hidden_dim, enc_dim)\n",
    "decoder = MLPDecoder(latent_dim+history_length, future_length, dec_dim)\n",
    "\n",
    "cvae = ContinuousCVAE(latent_dim, encoder, decoder, prior)\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 1E-3\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "z_samples = 16\n",
    "cvae.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    beta = beta_schedule((epoch + 1)/ num_epochs) # we slowly increase the weighting on the KL divergence, following https://openreview.net/forum?id=Sy2fzU9gl\n",
    "    for batch_idx, (history, future) in enumerate(train_dataloader):\n",
    "        q_mu, q_logvar, y_mu, y_logvar = cvae(history, future)\n",
    "        p_mu, p_logvar = cvae.encode_prior(history)\n",
    "        optimizer.zero_grad()\n",
    "        loss = cvae.elbo(history, future, z_samples, beta)\n",
    "        loss.backward()\n",
    "        running_loss += loss.detach().cpu().numpy()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'======= Epoch {epoch+1} completed with average loss: {running_loss/len(train_dataloader):.4f} =======')\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction on test data\n",
    "\n",
    "cvae.eval()\n",
    "\n",
    "num_samples = 8\n",
    "num_latent_samples = 8\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = cvae.sample(history, num_samples, num_latent_samples)\n",
    "    \n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_data)-1, step=1, description='Index:')\n",
    "xlims = [-12, 10]\n",
    "ylims = [-12, 12]\n",
    "interact(hw1_helper.plot_data_generative, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5 # choose an index you want to plot\n",
    "hw1_helper.plot_data_generative(history=history, future=future, prediction=prediction, index=index, xlims=xlims, ylims=ylims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteCVAE(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, importance, decoder, prior, num_categories):\n",
    "        '''\n",
    "        latent_dim: dimension of the continuous latent space\n",
    "        importance: network to encode the importance weight\n",
    "        decoder: network to decode the output\n",
    "        prior: network to encode the prior  \n",
    "        num_categories: number of categories per latent dimension \n",
    "        '''\n",
    "        \n",
    "        super(DiscreteCVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.importance = importance\n",
    "        self.decoder = decoder\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.logits_projection_encoder = torch.nn.Linear(importance.output_dim, latent_dim * num_categories)\n",
    "        self.mean_projection_decoder = torch.nn.Linear(decoder.output_dim, decoder.output_dim)\n",
    "        self.logvar_projection_decoder = torch.nn.Linear(decoder.output_dim, decoder.output_dim)\n",
    "        self.num_categories = num_categories\n",
    "        \n",
    "        # Gumbel-softmax reparameterization\n",
    "        self.gumbel_temperature = 0.1\n",
    "        \n",
    "    def encode_importance(self, x, y):\n",
    "        '''Computes logits of q(z|x,y), assumes one-hot categorical'''\n",
    "        xy = torch.cat([x, y], dim=-1)\n",
    "        h = self.importance(xy)\n",
    "        z_logits = self.logits_projection_encoder(h).reshape(-1, self.latent_dim, self.num_categories)      \n",
    "        return z_logits\n",
    "    \n",
    "    \n",
    "    def encode_prior(self, x):\n",
    "        '''Computes logits of p(z|x), assumes one-hot categorical'''\n",
    "        h = self.prior(x)\n",
    "        z_logits = self.logits_projection_encoder(h).reshape(-1, self.latent_dim, self.num_categories)\n",
    "        \n",
    "        return z_logits\n",
    "\n",
    "\n",
    "    def reparameterize(self, logits):\n",
    "        '''Sample latent variables using one-hot categorical distributions. Uses gumbel-softmax trick. Pytorch has a bulit-in function for this.'''\n",
    "        \n",
    "        return F.gumbel_softmax(logits, tau=self.gumbel_temperature, hard=True) \n",
    "        \n",
    "        \n",
    "    def decode(self, x, z):\n",
    "        '''Computes mean and log(covariance) of p(y|x,z), assumes normal distribution'''\n",
    "        xz = torch.cat([x, z], dim=-1)\n",
    "        g = self.decoder(xz)\n",
    "        y_mu = self.mean_projection_decoder(g)\n",
    "        y_logvar = torch.clip(self.logvar_projection_decoder(g), min=-10, max=1)\n",
    "        \n",
    "        return y_mu, y_logvar\n",
    "\n",
    "\n",
    "    def forward(self, x, y, n=1):\n",
    "        '''forward pass of the cvae model'''\n",
    "        #  get p(z|x,(y)) and samples from it n times\n",
    "        if self.training:\n",
    "            logits = self.encode_importance(x, y) # [bs, latent_dim, num_categories]\n",
    "            z = self.reparameterize(repeat_n(logits, n)) # [n, bs, latent_dim, num_categories]\n",
    "        else:\n",
    "            logits = self.encode_prior(x) # [bs, latent_dim, num_categories]\n",
    "            z = torch.distributions.OneHotCategorical(logits=logits).sample((n,)) # [n, bs, latent_dim, num_categories]\n",
    "        z_flatten = z.view(n, -1, self.latent_dim * self.num_categories)  # reshapes to [n, bs, latent_dim * num_categories]\n",
    "        # get p(y|x,z)\n",
    "        y_mu, y_logvar  = self.decode(repeat_n(x, n), z_flatten) \n",
    "\n",
    "        return logits, y_mu, y_logvar\n",
    "    \n",
    "    \n",
    "    def sample(self, x, num_samples=8, num_latent_samples=8):\n",
    "        '''samples from p(y|x,z) where z~p(z|x). Need to specify the number z and y samples to draw'''\n",
    "        _, y_mu, y_logvar = self.forward(x, None, num_latent_samples)\n",
    "\n",
    "        return  torch.distributions.MultivariateNormal(loc=y_mu, covariance_matrix=torch.diag_embed(torch.exp(y_logvar))).rsample((num_samples,))\n",
    "        \n",
    "        \n",
    "    def elbo(self, x, y, z_samples=1, beta=1.):\n",
    "        '''Compute ELBO for CVAE with discrete latent space. Optional: beta term that weigh kl divergence term'''\n",
    "\n",
    "        logits, y_mu, y_logvar = self.forward(x, y, z_samples)\n",
    "        prior_logits = cvae.encode_prior(x)\n",
    "        \n",
    "        y_prob = torch.distributions.MultivariateNormal(loc=y_mu, covariance_matrix=torch.diag_embed(torch.exp(y_logvar)))  # p(y|x, z)\n",
    "        \n",
    "        q_z = F.softmax(logits, dim=-1)  # q(z|x,y)\n",
    "        log_p_z = F.log_softmax(prior_logits, dim=-1)  # log(p(z|x))\n",
    "        \n",
    "        loglikelihood = -y_prob.log_prob(repeat_n(y, z_samples)).mean()\n",
    "        kl_div = torch.nn.KLDivLoss(reduction=\"batchmean\")(log_p_z, q_z)\n",
    "        \n",
    "        return loglikelihood + beta * kl_div\n",
    "      \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrete CVAE\n",
    "\n",
    "history_length = 11\n",
    "future_length = 10\n",
    "hidden_dim = 8\n",
    "enc_dim = 8\n",
    "dec_dim = 8\n",
    "\n",
    "latent_dim = 2\n",
    "num_categories = 3\n",
    "\n",
    "encoder = MLPEncoder(history_length + future_length, hidden_dim, enc_dim)\n",
    "prior = MLPEncoder(history_length, hidden_dim, enc_dim)\n",
    "decoder = MLPDecoder(latent_dim * num_categories + history_length, future_length, dec_dim)\n",
    "\n",
    "cvae = DiscreteCVAE(latent_dim, encoder, decoder, prior, num_categories)\n",
    "\n",
    "learning_rate = 1E-3\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=learning_rate, weight_decay=1E-2)\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "cvae.train()\n",
    "num_latent_samples = 8\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for batch_idx, (history, future) in enumerate(train_dataloader):\n",
    "        beta = beta_schedule((epoch+1) / num_epochs) # we slowly increase the weighting on the KL divergence, following https://openreview.net/forum?id=Sy2fzU9gl\n",
    "        optimizer.zero_grad()\n",
    "        loss = cvae.elbo(history, future, num_latent_samples, beta)\n",
    "        loss.backward()\n",
    "        running_loss += loss.detach().cpu().numpy()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'======= Epoch {epoch+1} completed with average loss: {running_loss/len(train_dataloader):.4f} =======')\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction on test data\n",
    "\n",
    "cvae.eval()\n",
    "num_latent_samples = 32\n",
    "num_samples = 1\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = cvae.sample(history, num_samples, num_latent_samples)\n",
    "    \n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_data)-1, step=1, description='Index:')\n",
    "xlims = [-12, 10]\n",
    "ylims = [-12, 12]\n",
    "interact(hw1_helper.plot_data_generative, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5 # choose an index you want to plot\n",
    "hw1_helper.plot_data_generative(history=history, future=future, prediction=prediction, index=index, xlims=xlims, ylims=ylims)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa598_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
